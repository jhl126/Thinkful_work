{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F4I54bZzMImI"
   },
   "source": [
    "# Machine Learning: Text Classification Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBhHdzKtMImK"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"from nltk import word_tokenize\\nfrom nltk.corpus import stopwords\\nfrom sklearn.pipeline import Pipeline\\nfrom nltk.stem.snowball import SnowballStemmer\\nfrom nltk.stem.wordnet import WordNetLemmatizer\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\\nfrom sklearn.model_selection import train_test_split as tts\\nfrom nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\\nfrom bs4 import BeautifulSoup\\n\\nimport requests\";\n",
       "                var nbb_formatted_code = \"from nltk import word_tokenize\\nfrom nltk.corpus import stopwords\\nfrom sklearn.pipeline import Pipeline\\nfrom nltk.stem.snowball import SnowballStemmer\\nfrom nltk.stem.wordnet import WordNetLemmatizer\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\\nfrom sklearn.model_selection import train_test_split as tts\\nfrom nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\\nfrom bs4 import BeautifulSoup\\n\\nimport requests\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d2tVDJJaMImN"
   },
   "source": [
    "### Use the CategorizedPlaintextCorpusReader to import the AP_News corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "REtoZb_iMImO"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# regex syntax\\nPATH = \\\"./AP_News/AP_News\\\"\\n\\nDOC_PATTERN = r\\\".*\\\\.txt\\\"\\nCAT_PATTERN = r\\\"([\\\\w_\\\\w]+)/.*\\\"\\n\\ncorpus = CategorizedPlaintextCorpusReader(PATH, DOC_PATTERN, cat_pattern=CAT_PATTERN)\";\n",
       "                var nbb_formatted_code = \"# regex syntax\\nPATH = \\\"./AP_News/AP_News\\\"\\n\\nDOC_PATTERN = r\\\".*\\\\.txt\\\"\\nCAT_PATTERN = r\\\"([\\\\w_\\\\w]+)/.*\\\"\\n\\ncorpus = CategorizedPlaintextCorpusReader(PATH, DOC_PATTERN, cat_pattern=CAT_PATTERN)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# regex syntax\n",
    "PATH = \"./AP_News/AP_News\"\n",
    "\n",
    "DOC_PATTERN = r\".*\\.txt\"\n",
    "CAT_PATTERN = r\"([\\w_\\w]+)/.*\"\n",
    "\n",
    "corpus = CategorizedPlaintextCorpusReader(PATH, DOC_PATTERN, cat_pattern=CAT_PATTERN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SQMuBvquMImP"
   },
   "source": [
    "### Create two separate lists - one containing the text from each document and another containing the category of each article in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8jU4ZNM-MImQ"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"docs = [corpus.raw(fileid) for fileid in corpus.fileids()]\\ncategories = [corpus.categories(fileid)[0] for fileid in corpus.fileids()]\";\n",
       "                var nbb_formatted_code = \"docs = [corpus.raw(fileid) for fileid in corpus.fileids()]\\ncategories = [corpus.categories(fileid)[0] for fileid in corpus.fileids()]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs = [corpus.raw(fileid) for fileid in corpus.fileids()]\n",
    "categories = [corpus.categories(fileid)[0] for fileid in corpus.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'health',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'politics',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'sports',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech',\n",
       " 'tech']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"categories\";\n",
       "                var nbb_formatted_code = \"categories\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGRABGW8MImR"
   },
   "source": [
    "### Preprocess the corpus, ensuring to include the following steps.\n",
    "\n",
    "- Word tokenize the documents.\n",
    "- Lemmatize, stem, and lowercase all tokens.\n",
    "- Remove punctuation and stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hk-Nlze1MImS"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# def preprocess(docs):\\n#     lemmatizer = WordNetLemmatizer()\\n#     stemmer = SnowballStemmer(\\\"english\\\")\\n#     preprocessed = []\\n#     for doc in docs:\\n#         tokenized = word_tokenize(doc)\\n#         cleaned = [\\n#             stemmer.stem(lemmatizer.lemmatize(token.lower()))\\n#             for token in tokenized\\n#             if not token.lower() in stopwords.words(\\\"english\\\")\\n#             if token.isalpha()\\n#         ]\\n#         untokenized = \\\" \\\".join(cleaned)\\n#         preprocessed.append(untokenized)\\n#     return preprocessed\";\n",
       "                var nbb_formatted_code = \"# def preprocess(docs):\\n#     lemmatizer = WordNetLemmatizer()\\n#     stemmer = SnowballStemmer(\\\"english\\\")\\n#     preprocessed = []\\n#     for doc in docs:\\n#         tokenized = word_tokenize(doc)\\n#         cleaned = [\\n#             stemmer.stem(lemmatizer.lemmatize(token.lower()))\\n#             for token in tokenized\\n#             if not token.lower() in stopwords.words(\\\"english\\\")\\n#             if token.isalpha()\\n#         ]\\n#         untokenized = \\\" \\\".join(cleaned)\\n#         preprocessed.append(untokenized)\\n#     return preprocessed\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(docs):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    preprocessed = []\n",
    "    for doc in docs:\n",
    "        tokenized = word_tokenize(doc)\n",
    "        cleaned = [\n",
    "            stemmer.stem(lemmatizer.lemmatize(token.lower()))\n",
    "            for token in tokenized\n",
    "            if not token.lower() in stopwords.words(\"english\")\n",
    "            if token.isalpha()\n",
    "        ]\n",
    "        untokenized = \" \".join(cleaned)\n",
    "        preprocessed.append(untokenized)\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# preprocessed = preprocess(docs)\";\n",
       "                var nbb_formatted_code = \"# preprocessed = preprocess(docs)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed = preprocess(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gZE-q4ziMImT"
   },
   "source": [
    "### Split the data into training and testing sets with the size of the test set being 30% of the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hKFyBgjBMImU"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"X_train, X_test, y_train, y_test = tts(docs, categories, test_size=0.3)\";\n",
       "                var nbb_formatted_code = \"X_train, X_test, y_train, y_test = tts(docs, categories, test_size=0.3)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = tts(docs, categories, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "reegQu_5MImV"
   },
   "source": [
    "### Construct a pipeline that TF-IDF vectorizes the text and trains a Random Forest classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3JJ3hjNMImW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='...\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=None, max_features='auto',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"pipe = Pipeline([(\\\"tfidf\\\", TfidfVectorizer()), (\\\"rf\\\", RandomForestClassifier())])\\npipe.fit(X_train, y_train)\";\n",
       "                var nbb_formatted_code = \"pipe = Pipeline([(\\\"tfidf\\\", TfidfVectorizer()), (\\\"rf\\\", RandomForestClassifier())])\\npipe.fit(X_train, y_train)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"rf\", RandomForestClassifier())])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cFt6djjpMImX"
   },
   "source": [
    "### Generate predictions on the test set and print a classification report to evaluate how well the model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWT99tvHMImY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      health       0.63      0.80      0.71        15\n",
      "    politics       0.87      0.72      0.79        18\n",
      "      sports       0.74      0.93      0.82        15\n",
      "        tech       0.85      0.61      0.71        18\n",
      "\n",
      "    accuracy                           0.76        66\n",
      "   macro avg       0.77      0.77      0.76        66\n",
      "weighted avg       0.78      0.76      0.76        66\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"pred = pipe.predict(X_test)\\nprint(classification_report(y_test, pred))\";\n",
       "                var nbb_formatted_code = \"pred = pipe.predict(X_test)\\nprint(classification_report(y_test, pred))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2BwKeBNMImZ"
   },
   "source": [
    "### Perform 10-fold cross validation and obtain the averge F1 score across all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-n9cTvc6MIma"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ceca085b9d44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"f1_macro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessed' is not defined"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"scores = cross_val_score(pipe, preprocessed, categories, cv=10, scoring=\\\"f1_macro\\\")\";\n",
       "                var nbb_formatted_code = \"scores = cross_val_score(pipe, preprocessed, categories, cv=10, scoring=\\\"f1_macro\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = cross_val_score(pipe, preprocessed, categories, cv=10, scoring=\"f1_macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1H9ERShiMImb"
   },
   "source": [
    "### Ingest, preprocess, and predict the topic of the article at the following URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_text(url):\n",
    "    response = requests.get(url)\n",
    "    content = response.text\n",
    "\n",
    "    TAGS = [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"h7\", \"p\", \"li\"]\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    text_list = [tag.get_text() for tag in soup.find_all(TAGS)]\n",
    "    text = \" \".join(text_list)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0d2mfYGSMImc"
   },
   "outputs": [],
   "source": [
    "url = \"https://www.nytimes.com/2019/11/25/business/uber-london.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_url_text(url)\n",
    "cleaned = preprocess([text])\n",
    "pipe.predict(cleaned)[0]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Day 74, Lecture 1: Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
