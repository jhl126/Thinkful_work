{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IihPCGvd34PH"
   },
   "source": [
    "# Text Acquisition & Ingestion Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feedparser\n",
      "  Downloading feedparser-6.0.2-py3-none-any.whl (80 kB)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6071 sha256=badea3178ad417c5e86ff9057a324797b337c794116ff5584c431ffff65776a9\n",
      "  Stored in directory: c:\\users\\jlim7\\appdata\\local\\pip\\cache\\wheels\\73\\ad\\a4\\0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser\n",
      "Successfully installed feedparser-6.0.2 sgmllib3k-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhLA_w7p34PM"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import feedparser\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZ3ytdXA34PR"
   },
   "source": [
    "### Iterate through the list of article URLs below, scraping the text from each one and saving it to a text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mDbZqPpT34PU"
   },
   "outputs": [],
   "source": [
    "articles = ['http://lite.cnn.io/en/article/h_eac18760a7a7f9a1bf33616f1c4a336d',\n",
    "            'http://lite.cnn.io/en/article/h_de3f82f17d289680dd2b47c6413ebe7c',\n",
    "            'http://lite.cnn.io/en/article/h_72f4dc9d6f35458a89af014b62e625ad',\n",
    "            'http://lite.cnn.io/en/article/h_aa21fe6bf176071cb49e09d422c3adf0',\n",
    "            'http://lite.cnn.io/en/article/h_8ad34a532921c9076cdc9d7390d2f1bc',\n",
    "            'http://lite.cnn.io/en/article/h_84422c79110d9989177cfaf1c5f45fe7',\n",
    "            'http://lite.cnn.io/en/article/h_d010d9580abac3a44c6181ec6fb63d58',\n",
    "            'http://lite.cnn.io/en/article/h_fb11f4e9d7c5323e75b337d9e9e5e368',\n",
    "            'http://lite.cnn.io/en/article/h_7b27f0b131067f8ece6238ac559670ab',\n",
    "            'http://lite.cnn.io/en/article/h_8cae7f735fa9573d470f802063ceffe2',\n",
    "            'http://lite.cnn.io/en/article/h_72c3668280e82576fcc2602b0fa70c14',\n",
    "            'http://lite.cnn.io/en/article/h_d20658fb0e20212051cda0e0a7248c8a',\n",
    "            'http://lite.cnn.io/en/article/h_56611c43d7928120d2ae21666ccc7417',\n",
    "            'http://lite.cnn.io/en/article/h_bda0394e3c5ee7054ee65c022bca7695']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AfDWUtDr34PX"
   },
   "outputs": [],
   "source": [
    "for article in articles:\n",
    "    response = requests.get(article)\n",
    "    content = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BgGlAM2X34Pc"
   },
   "source": [
    "### Ingest the text files generated via web scraping into a corpus and print the corpus statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mmybAnNB34Pf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sGP_4yfR34Pi"
   },
   "source": [
    "### Parse the O'Reilly Radar RSS feed below, extract the text from each post, and save it to a text file.\n",
    "\n",
    "The content of each post contains HTML tags. Strip those out using the same approach you used for web scraping so that only text is saved to the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDbqkFCF34Pl"
   },
   "outputs": [],
   "source": [
    "feed = 'http://feeds.feedburner.com/oreilly/radar/atom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l-a9JgWH34Po"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SqK4w9sa34Pr"
   },
   "source": [
    "### Ingest the text files generated via RSS parsing into a corpus and print the corpus statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cd4x36-934Ps"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9BDwjO8t34Pw"
   },
   "source": [
    "### Make an API call to the Hacker News API to retrieve their Ask, Show, and Job category items. \n",
    "\n",
    "- URL: https://hacker-news.firebaseio.com/v0/askstories.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7vyWOSN34Px"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yEasqCkr34P0"
   },
   "source": [
    "### Once you have retrieved the item IDs from the URL above, retrieve each item by adding the item ID to the URL below, extract the item's text property, and save the text from each item to disk as its own document.\n",
    "\n",
    "- URL: https://hacker-news.firebaseio.com/v0/item/ITEM_ID_HERE.json\n",
    "\n",
    "The content of some items may contain HTML tags. Strip those out using the same approach you used for web scraping so that only text is saved to the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Vp7yfWB34P1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hw9-v8sK34P5"
   },
   "source": [
    "### Ingest the text files generated via API into a corpus and print the corpus statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dcsiPdkx34P7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Day 71, Lecture 2: Afternoon Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
