{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tEyRSqh_0jS7"
   },
   "source": [
    "## Day 82 Lecture 1 Assignment\n",
    "\n",
    "In this assignment, we will learn about activation functions. We will create a neural network and measure the model's performance using different activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DgLFu1Hs0jS-"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split as tts\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\";\n",
       "                var nbb_formatted_code = \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split as tts\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTzJti900jTA"
   },
   "source": [
    "We will import the famous titanic dataset below and produce a neural network that will predict the chance of survival for a passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDlLGz270jTB"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"titanic = pd.read_csv(\\\"./titanic.csv\\\")\";\n",
       "                var nbb_formatted_code = \"titanic = pd.read_csv(\\\"./titanic.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic = pd.read_csv(\"./titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z01i6-rV0jTD",
    "outputId": "3002a433-b5e6-4fbb-85cb-214d4981ec19"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"titanic.head()\";\n",
       "                var nbb_formatted_code = \"titanic.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xyD_6NR00jTG"
   },
   "source": [
    "We'll perform some feature engineering\n",
    "\n",
    "Let's start by keeping only the columns we'd like to use for our analysis. Keep only the columns: Survived, Pclass, Sex, SibSp, Parch, and Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lfp0XlLk0jTG"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex  SibSp  Parch Embarked\n",
       "0         0       3    male      1      0        S\n",
       "1         1       1  female      1      0        C\n",
       "2         1       3  female      0      0        S\n",
       "3         1       1  female      1      0        S\n",
       "4         0       3    male      0      0        S"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# Answer below:\\ntitanic = titanic.drop(\\n    columns=[\\\"PassengerId\\\", \\\"Name\\\", \\\"Age\\\", \\\"Ticket\\\", \\\"Fare\\\", \\\"Cabin\\\"]\\n)\\ntitanic.head()\";\n",
       "                var nbb_formatted_code = \"# Answer below:\\ntitanic = titanic.drop(\\n    columns=[\\\"PassengerId\\\", \\\"Name\\\", \\\"Age\\\", \\\"Ticket\\\", \\\"Fare\\\", \\\"Cabin\\\"]\\n)\\ntitanic.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below:\n",
    "titanic = titanic.drop(\n",
    "    columns=[\"PassengerId\", \"Name\", \"Age\", \"Ticket\", \"Fare\", \"Cabin\"]\n",
    ")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvBUzjzG0jTI"
   },
   "source": [
    "Now examine how many rows contain missing data. Given how much missing data we have, should we remove the column with the most missing data, or remove all rows containing missing data? Do what you think is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PwnHzNjP0jTJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived    0.000000\n",
       "Pclass      0.000000\n",
       "Sex         0.000000\n",
       "SibSp       0.000000\n",
       "Parch       0.000000\n",
       "Embarked    0.002245\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# Answer below:\\n# seeing how there is less than 1% of missing data, we might as well just remove the rows\\ntitanic.isna().mean()\";\n",
       "                var nbb_formatted_code = \"# Answer below:\\n# seeing how there is less than 1% of missing data, we might as well just remove the rows\\ntitanic.isna().mean()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below:\n",
    "# seeing how there is less than 1% of missing data, we might as well just remove the rows\n",
    "titanic.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived    0.0\n",
       "Pclass      0.0\n",
       "Sex         0.0\n",
       "SibSp       0.0\n",
       "Parch       0.0\n",
       "Embarked    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"titanic = titanic.dropna()\\ntitanic.isna().mean()\";\n",
       "                var nbb_formatted_code = \"titanic = titanic.dropna()\\ntitanic.isna().mean()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic = titanic.dropna()\n",
    "titanic.isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "imTrmBpL0jTK"
   },
   "source": [
    "Now we'll create a one hot encoding of the variables Pclass, sex, and Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SD_qCdv60jTL",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  SibSp  Parch  Pclass_2  Pclass_3  Sex_male  Embarked_Q  \\\n",
       "0         0      1      0         0         1         1           0   \n",
       "1         1      1      0         0         0         0           0   \n",
       "2         1      0      0         0         1         0           0   \n",
       "3         1      1      0         0         0         0           0   \n",
       "4         0      0      0         0         1         1           0   \n",
       "\n",
       "   Embarked_S  \n",
       "0           1  \n",
       "1           0  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# Answer below:\\ntitanic = pd.get_dummies(\\n    data=titanic, columns=[\\\"Pclass\\\", \\\"Sex\\\", \\\"Embarked\\\"], drop_first=True\\n)\\ntitanic.head()\";\n",
       "                var nbb_formatted_code = \"# Answer below:\\ntitanic = pd.get_dummies(\\n    data=titanic, columns=[\\\"Pclass\\\", \\\"Sex\\\", \\\"Embarked\\\"], drop_first=True\\n)\\ntitanic.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below:\n",
    "titanic = pd.get_dummies(\n",
    "    data=titanic, columns=[\"Pclass\", \"Sex\", \"Embarked\"], drop_first=True\n",
    ")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5vv2pncX0jTM"
   },
   "source": [
    "Split the data into train and test. 20% of the data should be set aside for testing. Use Survived as your target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DzzK0j-0jTN"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"# Answer below\\nX_train, X_test, y_train, y_test = tts(titanic.drop(columns = ['Survived']), titanic['Survived'],test_size = 0.20, random_state = 13)\";\n",
       "                var nbb_formatted_code = \"# Answer below\\nX_train, X_test, y_train, y_test = tts(\\n    titanic.drop(columns=[\\\"Survived\\\"]),\\n    titanic[\\\"Survived\\\"],\\n    test_size=0.20,\\n    random_state=13,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below\n",
    "X_train, X_test, y_train, y_test = tts(titanic.drop(columns = ['Survived']), titanic['Survived'],test_size = 0.20, random_state = 13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-27Xq4M0jTP"
   },
   "source": [
    "At this point, we are ready to create a model. Import `Sequential` and `Dense` from Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ub0uCwyH0jTR"
   },
   "source": [
    "Create a model with 5 layers. The first layer should be a dense layer that receives the input, the last layer should be of size 1. You determine the remaining layer sizes.\n",
    "\n",
    "Use a tanh activation for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wSiTyR7D0jTR"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"# Answer below\\nmodel = Sequential()\\nmodel.add(Dense(64, input_dim=X_train.shape[1], activation=\\\"relu\\\"))\\nmodel.add(Dense(32, activation=\\\"relu\\\"))\\nmodel.add(Dense(16, activation=\\\"relu\\\"))\\nmodel.add(Dense(8, activation=\\\"relu\\\"))\\nmodel.add(Dense(1, activation=\\\"tanh\\\"))\";\n",
       "                var nbb_formatted_code = \"# Answer below\\nmodel = Sequential()\\nmodel.add(Dense(64, input_dim=X_train.shape[1], activation=\\\"relu\\\"))\\nmodel.add(Dense(32, activation=\\\"relu\\\"))\\nmodel.add(Dense(16, activation=\\\"relu\\\"))\\nmodel.add(Dense(8, activation=\\\"relu\\\"))\\nmodel.add(Dense(1, activation=\\\"tanh\\\"))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation=\"relu\"))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"tanh\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yh-Ely-K0jTT"
   },
   "source": [
    "Compile the model using the adam optimizer, binary crossentropy loss, and the accuracy metric.\n",
    "\n",
    "Fit the model using a batch size of 80 over 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lT-9MGP30jTT"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"# Answer below:\\nmodel.compile(optimizer=\\\"adam\\\", loss=\\\"binary_crossentropy\\\", metrics=[\\\"accuracy\\\"])\";\n",
       "                var nbb_formatted_code = \"# Answer below:\\nmodel.compile(optimizer=\\\"adam\\\", loss=\\\"binary_crossentropy\\\", metrics=[\\\"accuracy\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below:\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "WARNING:tensorflow:From C:\\Users\\jlim7\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 711 samples, validate on 178 samples\n",
      "Epoch 1/300\n",
      "711/711 [==============================] - 0s 200us/sample - loss: 0.8397 - accuracy: 0.5823 - val_loss: 0.7719 - val_accuracy: 0.5562\n",
      "Epoch 2/300\n",
      "711/711 [==============================] - 0s 18us/sample - loss: 0.7037 - accuracy: 0.6048 - val_loss: 0.7006 - val_accuracy: 0.6292\n",
      "Epoch 3/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.6436 - accuracy: 0.6540 - val_loss: 0.6415 - val_accuracy: 0.6517\n",
      "Epoch 4/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.5938 - accuracy: 0.6962 - val_loss: 0.5827 - val_accuracy: 0.7079\n",
      "Epoch 5/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.5526 - accuracy: 0.7454 - val_loss: 0.5424 - val_accuracy: 0.7416\n",
      "Epoch 6/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.5220 - accuracy: 0.7553 - val_loss: 0.5193 - val_accuracy: 0.7640\n",
      "Epoch 7/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.4981 - accuracy: 0.7623 - val_loss: 0.4960 - val_accuracy: 0.8034\n",
      "Epoch 8/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.4849 - accuracy: 0.7722 - val_loss: 0.4864 - val_accuracy: 0.7697\n",
      "Epoch 9/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.4706 - accuracy: 0.7778 - val_loss: 0.4679 - val_accuracy: 0.8090\n",
      "Epoch 10/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.4636 - accuracy: 0.7932 - val_loss: 0.4649 - val_accuracy: 0.7921\n",
      "Epoch 11/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.4548 - accuracy: 0.8017 - val_loss: 0.4632 - val_accuracy: 0.7921\n",
      "Epoch 12/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.4463 - accuracy: 0.8059 - val_loss: 0.4522 - val_accuracy: 0.7865\n",
      "Epoch 13/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.4393 - accuracy: 0.8073 - val_loss: 0.4535 - val_accuracy: 0.7865\n",
      "Epoch 14/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.4383 - accuracy: 0.8115 - val_loss: 0.4495 - val_accuracy: 0.7809\n",
      "Epoch 15/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.4292 - accuracy: 0.8073 - val_loss: 0.4389 - val_accuracy: 0.7809\n",
      "Epoch 16/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.4286 - accuracy: 0.8059 - val_loss: 0.4456 - val_accuracy: 0.7921\n",
      "Epoch 17/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.4252 - accuracy: 0.8129 - val_loss: 0.4406 - val_accuracy: 0.7978\n",
      "Epoch 18/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.4242 - accuracy: 0.8143 - val_loss: 0.4438 - val_accuracy: 0.7921\n",
      "Epoch 19/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.4175 - accuracy: 0.8101 - val_loss: 0.4399 - val_accuracy: 0.7978\n",
      "Epoch 20/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.4163 - accuracy: 0.8172 - val_loss: 0.4413 - val_accuracy: 0.7978\n",
      "Epoch 21/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.4138 - accuracy: 0.8186 - val_loss: 0.4347 - val_accuracy: 0.8034\n",
      "Epoch 22/300\n",
      "711/711 [==============================] - 0s 31us/sample - loss: 0.4098 - accuracy: 0.8200 - val_loss: 0.4417 - val_accuracy: 0.7978\n",
      "Epoch 23/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.4138 - accuracy: 0.8242 - val_loss: 0.4391 - val_accuracy: 0.8146\n",
      "Epoch 24/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.4098 - accuracy: 0.8200 - val_loss: 0.4433 - val_accuracy: 0.8034\n",
      "Epoch 25/300\n",
      "711/711 [==============================] - 0s 35us/sample - loss: 0.4055 - accuracy: 0.8256 - val_loss: 0.4428 - val_accuracy: 0.8146\n",
      "Epoch 26/300\n",
      "711/711 [==============================] - 0s 31us/sample - loss: 0.4028 - accuracy: 0.8256 - val_loss: 0.4385 - val_accuracy: 0.8090\n",
      "Epoch 27/300\n",
      "711/711 [==============================] - 0s 31us/sample - loss: 0.4038 - accuracy: 0.8214 - val_loss: 0.4373 - val_accuracy: 0.8090\n",
      "Epoch 28/300\n",
      "711/711 [==============================] - 0s 35us/sample - loss: 0.4039 - accuracy: 0.8256 - val_loss: 0.4410 - val_accuracy: 0.8090\n",
      "Epoch 29/300\n",
      "711/711 [==============================] - 0s 38us/sample - loss: 0.4061 - accuracy: 0.8214 - val_loss: 0.4449 - val_accuracy: 0.8090\n",
      "Epoch 30/300\n",
      "711/711 [==============================] - 0s 35us/sample - loss: 0.4022 - accuracy: 0.8270 - val_loss: 0.4346 - val_accuracy: 0.8034\n",
      "Epoch 31/300\n",
      "711/711 [==============================] - 0s 31us/sample - loss: 0.4003 - accuracy: 0.8284 - val_loss: 0.4390 - val_accuracy: 0.8146\n",
      "Epoch 32/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.4003 - accuracy: 0.8242 - val_loss: 0.4427 - val_accuracy: 0.8090\n",
      "Epoch 33/300\n",
      "711/711 [==============================] - 0s 36us/sample - loss: 0.3974 - accuracy: 0.8270 - val_loss: 0.4360 - val_accuracy: 0.8146\n",
      "Epoch 34/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.3965 - accuracy: 0.8242 - val_loss: 0.4441 - val_accuracy: 0.8146\n",
      "Epoch 35/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.3954 - accuracy: 0.8256 - val_loss: 0.4333 - val_accuracy: 0.8090\n",
      "Epoch 36/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.4064 - accuracy: 0.8256 - val_loss: 0.4354 - val_accuracy: 0.8090\n",
      "Epoch 37/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.4029 - accuracy: 0.8298 - val_loss: 0.4380 - val_accuracy: 0.8202\n",
      "Epoch 38/300\n",
      "711/711 [==============================] - 0s 42us/sample - loss: 0.4006 - accuracy: 0.8256 - val_loss: 0.4476 - val_accuracy: 0.8090\n",
      "Epoch 39/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3979 - accuracy: 0.8284 - val_loss: 0.4381 - val_accuracy: 0.8202\n",
      "Epoch 40/300\n",
      "711/711 [==============================] - 0s 31us/sample - loss: 0.3921 - accuracy: 0.8340 - val_loss: 0.4392 - val_accuracy: 0.8090\n",
      "Epoch 41/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3913 - accuracy: 0.8284 - val_loss: 0.4391 - val_accuracy: 0.8146\n",
      "Epoch 42/300\n",
      "711/711 [==============================] - 0s 31us/sample - loss: 0.3889 - accuracy: 0.8312 - val_loss: 0.4409 - val_accuracy: 0.8090\n",
      "Epoch 43/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3928 - accuracy: 0.8284 - val_loss: 0.4389 - val_accuracy: 0.8202\n",
      "Epoch 44/300\n",
      "711/711 [==============================] - 0s 38us/sample - loss: 0.3901 - accuracy: 0.8284 - val_loss: 0.4412 - val_accuracy: 0.8146\n",
      "Epoch 45/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.3892 - accuracy: 0.8312 - val_loss: 0.4419 - val_accuracy: 0.8146\n",
      "Epoch 46/300\n",
      "711/711 [==============================] - 0s 38us/sample - loss: 0.3878 - accuracy: 0.8242 - val_loss: 0.4381 - val_accuracy: 0.8090\n",
      "Epoch 47/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3890 - accuracy: 0.8326 - val_loss: 0.4490 - val_accuracy: 0.8146\n",
      "Epoch 48/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.3905 - accuracy: 0.8340 - val_loss: 0.4470 - val_accuracy: 0.8202\n",
      "Epoch 49/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.3851 - accuracy: 0.8326 - val_loss: 0.4367 - val_accuracy: 0.8090\n",
      "Epoch 50/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.3852 - accuracy: 0.8340 - val_loss: 0.4443 - val_accuracy: 0.8146\n",
      "Epoch 51/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3854 - accuracy: 0.8340 - val_loss: 0.4496 - val_accuracy: 0.8090\n",
      "Epoch 52/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3874 - accuracy: 0.8368 - val_loss: 0.4589 - val_accuracy: 0.8034\n",
      "Epoch 53/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3872 - accuracy: 0.8326 - val_loss: 0.4536 - val_accuracy: 0.8090\n",
      "Epoch 54/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3859 - accuracy: 0.8354 - val_loss: 0.4503 - val_accuracy: 0.8202\n",
      "Epoch 55/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.3865 - accuracy: 0.8354 - val_loss: 0.5082 - val_accuracy: 0.8034\n",
      "Epoch 56/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3850 - accuracy: 0.8326 - val_loss: 0.4417 - val_accuracy: 0.8202\n",
      "Epoch 57/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.3837 - accuracy: 0.8411 - val_loss: 0.5024 - val_accuracy: 0.8146\n",
      "Epoch 58/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3841 - accuracy: 0.8383 - val_loss: 0.4964 - val_accuracy: 0.8146\n",
      "Epoch 59/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3828 - accuracy: 0.8368 - val_loss: 0.5077 - val_accuracy: 0.8034\n",
      "Epoch 60/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3809 - accuracy: 0.8383 - val_loss: 0.4984 - val_accuracy: 0.8202\n",
      "Epoch 61/300\n",
      "711/711 [==============================] - 0s 31us/sample - loss: 0.3835 - accuracy: 0.8411 - val_loss: 0.4535 - val_accuracy: 0.8202\n",
      "Epoch 62/300\n",
      "711/711 [==============================] - 0s 45us/sample - loss: 0.3801 - accuracy: 0.8368 - val_loss: 0.4480 - val_accuracy: 0.8090\n",
      "Epoch 63/300\n",
      "711/711 [==============================] - 0s 31us/sample - loss: 0.3818 - accuracy: 0.8383 - val_loss: 0.4998 - val_accuracy: 0.8090\n",
      "Epoch 64/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3821 - accuracy: 0.8383 - val_loss: 0.5659 - val_accuracy: 0.8090\n",
      "Epoch 65/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3826 - accuracy: 0.8354 - val_loss: 0.4605 - val_accuracy: 0.8034\n",
      "Epoch 66/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3830 - accuracy: 0.8368 - val_loss: 0.5160 - val_accuracy: 0.8090\n",
      "Epoch 67/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3846 - accuracy: 0.8326 - val_loss: 0.4567 - val_accuracy: 0.8034\n",
      "Epoch 68/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3834 - accuracy: 0.8368 - val_loss: 0.4380 - val_accuracy: 0.8034\n",
      "Epoch 69/300\n",
      "711/711 [==============================] - 0s 31us/sample - loss: 0.3804 - accuracy: 0.8368 - val_loss: 0.5619 - val_accuracy: 0.7978\n",
      "Epoch 70/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3853 - accuracy: 0.8354 - val_loss: 0.5223 - val_accuracy: 0.7978\n",
      "Epoch 71/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3903 - accuracy: 0.8312 - val_loss: 0.4948 - val_accuracy: 0.8034\n",
      "Epoch 72/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3813 - accuracy: 0.8368 - val_loss: 0.5101 - val_accuracy: 0.8034\n",
      "Epoch 73/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3821 - accuracy: 0.8326 - val_loss: 0.5621 - val_accuracy: 0.7978\n",
      "Epoch 74/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3783 - accuracy: 0.8368 - val_loss: 0.5022 - val_accuracy: 0.8090\n",
      "Epoch 75/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.3794 - accuracy: 0.8397 - val_loss: 0.5021 - val_accuracy: 0.7978\n",
      "Epoch 76/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3779 - accuracy: 0.8397 - val_loss: 0.5708 - val_accuracy: 0.8090\n",
      "Epoch 77/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3814 - accuracy: 0.8368 - val_loss: 0.5585 - val_accuracy: 0.7978\n",
      "Epoch 78/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.3767 - accuracy: 0.8411 - val_loss: 0.5699 - val_accuracy: 0.8034\n",
      "Epoch 79/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3772 - accuracy: 0.8411 - val_loss: 0.5701 - val_accuracy: 0.8146\n",
      "Epoch 80/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3772 - accuracy: 0.8397 - val_loss: 0.5018 - val_accuracy: 0.8034\n",
      "Epoch 81/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3788 - accuracy: 0.8368 - val_loss: 0.5036 - val_accuracy: 0.8090\n",
      "Epoch 82/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3766 - accuracy: 0.8383 - val_loss: 0.5613 - val_accuracy: 0.8090\n",
      "Epoch 83/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3802 - accuracy: 0.8354 - val_loss: 0.5085 - val_accuracy: 0.8090\n",
      "Epoch 84/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3786 - accuracy: 0.8340 - val_loss: 0.5091 - val_accuracy: 0.8034\n",
      "Epoch 85/300\n",
      "711/711 [==============================] - 0s 26us/sample - loss: 0.3758 - accuracy: 0.8383 - val_loss: 0.5643 - val_accuracy: 0.8034\n",
      "Epoch 86/300\n",
      "711/711 [==============================] - 0s 23us/sample - loss: 0.3774 - accuracy: 0.8439 - val_loss: 0.5593 - val_accuracy: 0.7978\n",
      "Epoch 87/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3811 - accuracy: 0.8312 - val_loss: 0.5735 - val_accuracy: 0.7865\n",
      "Epoch 88/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3822 - accuracy: 0.8383 - val_loss: 0.5603 - val_accuracy: 0.8090\n",
      "Epoch 89/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3804 - accuracy: 0.8312 - val_loss: 0.5687 - val_accuracy: 0.8034\n",
      "Epoch 90/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3777 - accuracy: 0.8397 - val_loss: 0.5668 - val_accuracy: 0.7978\n",
      "Epoch 91/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3768 - accuracy: 0.8383 - val_loss: 0.5086 - val_accuracy: 0.7978\n",
      "Epoch 92/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3775 - accuracy: 0.8383 - val_loss: 0.5108 - val_accuracy: 0.8090\n",
      "Epoch 93/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3759 - accuracy: 0.8326 - val_loss: 0.5637 - val_accuracy: 0.8090\n",
      "Epoch 94/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3737 - accuracy: 0.8383 - val_loss: 0.5587 - val_accuracy: 0.8034\n",
      "Epoch 95/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3754 - accuracy: 0.8411 - val_loss: 0.5647 - val_accuracy: 0.7978\n",
      "Epoch 96/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3763 - accuracy: 0.8354 - val_loss: 0.5078 - val_accuracy: 0.8146\n",
      "Epoch 97/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3777 - accuracy: 0.8425 - val_loss: 0.5860 - val_accuracy: 0.7978\n",
      "Epoch 98/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3747 - accuracy: 0.8397 - val_loss: 0.5699 - val_accuracy: 0.8034\n",
      "Epoch 99/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3774 - accuracy: 0.8368 - val_loss: 0.5686 - val_accuracy: 0.7978\n",
      "Epoch 100/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3729 - accuracy: 0.8368 - val_loss: 0.5668 - val_accuracy: 0.8146\n",
      "Epoch 101/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3754 - accuracy: 0.8383 - val_loss: 0.5154 - val_accuracy: 0.7978\n",
      "Epoch 102/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3753 - accuracy: 0.8368 - val_loss: 0.5699 - val_accuracy: 0.8034\n",
      "Epoch 103/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.3744 - accuracy: 0.8397 - val_loss: 0.5674 - val_accuracy: 0.7978\n",
      "Epoch 104/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.3774 - accuracy: 0.8368 - val_loss: 0.5657 - val_accuracy: 0.8034\n",
      "Epoch 105/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3755 - accuracy: 0.8312 - val_loss: 0.5114 - val_accuracy: 0.7978\n",
      "Epoch 106/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.3802 - accuracy: 0.8368 - val_loss: 0.5087 - val_accuracy: 0.8146\n",
      "Epoch 107/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3763 - accuracy: 0.8383 - val_loss: 0.5778 - val_accuracy: 0.7921\n",
      "Epoch 108/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3754 - accuracy: 0.8411 - val_loss: 0.5639 - val_accuracy: 0.7978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3752 - accuracy: 0.8383 - val_loss: 0.5008 - val_accuracy: 0.8146\n",
      "Epoch 110/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3728 - accuracy: 0.8383 - val_loss: 0.5244 - val_accuracy: 0.7978\n",
      "Epoch 111/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3720 - accuracy: 0.8397 - val_loss: 0.5771 - val_accuracy: 0.7978\n",
      "Epoch 112/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3719 - accuracy: 0.8425 - val_loss: 0.5669 - val_accuracy: 0.8034\n",
      "Epoch 113/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3740 - accuracy: 0.8368 - val_loss: 0.5643 - val_accuracy: 0.7978\n",
      "Epoch 114/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3735 - accuracy: 0.8383 - val_loss: 0.5246 - val_accuracy: 0.8034\n",
      "Epoch 115/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3717 - accuracy: 0.8397 - val_loss: 0.5745 - val_accuracy: 0.8146\n",
      "Epoch 116/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3720 - accuracy: 0.8383 - val_loss: 0.5696 - val_accuracy: 0.8090\n",
      "Epoch 117/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3731 - accuracy: 0.8354 - val_loss: 0.5692 - val_accuracy: 0.7978\n",
      "Epoch 118/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3738 - accuracy: 0.8397 - val_loss: 0.5732 - val_accuracy: 0.8146\n",
      "Epoch 119/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3742 - accuracy: 0.8354 - val_loss: 0.5746 - val_accuracy: 0.8146\n",
      "Epoch 120/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3727 - accuracy: 0.8368 - val_loss: 0.5726 - val_accuracy: 0.8090\n",
      "Epoch 121/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3744 - accuracy: 0.8354 - val_loss: 0.5669 - val_accuracy: 0.8034\n",
      "Epoch 122/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3753 - accuracy: 0.8368 - val_loss: 0.5701 - val_accuracy: 0.8146\n",
      "Epoch 123/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3705 - accuracy: 0.8368 - val_loss: 0.5718 - val_accuracy: 0.8090\n",
      "Epoch 124/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3720 - accuracy: 0.8411 - val_loss: 0.5681 - val_accuracy: 0.8034\n",
      "Epoch 125/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3743 - accuracy: 0.8383 - val_loss: 0.5679 - val_accuracy: 0.8090\n",
      "Epoch 126/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3690 - accuracy: 0.8411 - val_loss: 0.5707 - val_accuracy: 0.7978\n",
      "Epoch 127/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3730 - accuracy: 0.8368 - val_loss: 0.5718 - val_accuracy: 0.7921\n",
      "Epoch 128/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3727 - accuracy: 0.8397 - val_loss: 0.5690 - val_accuracy: 0.7978\n",
      "Epoch 129/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3714 - accuracy: 0.8354 - val_loss: 0.5093 - val_accuracy: 0.8146\n",
      "Epoch 130/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3707 - accuracy: 0.8411 - val_loss: 0.5363 - val_accuracy: 0.8034\n",
      "Epoch 131/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3692 - accuracy: 0.8425 - val_loss: 0.5760 - val_accuracy: 0.7978\n",
      "Epoch 132/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3701 - accuracy: 0.8425 - val_loss: 0.5706 - val_accuracy: 0.8034\n",
      "Epoch 133/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3702 - accuracy: 0.8411 - val_loss: 0.5719 - val_accuracy: 0.8034\n",
      "Epoch 134/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3687 - accuracy: 0.8397 - val_loss: 0.5714 - val_accuracy: 0.8146\n",
      "Epoch 135/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3700 - accuracy: 0.8397 - val_loss: 0.5769 - val_accuracy: 0.7978\n",
      "Epoch 136/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.3735 - accuracy: 0.8368 - val_loss: 0.5288 - val_accuracy: 0.7978\n",
      "Epoch 137/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.3698 - accuracy: 0.8383 - val_loss: 0.5742 - val_accuracy: 0.8034\n",
      "Epoch 138/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3693 - accuracy: 0.8368 - val_loss: 0.5688 - val_accuracy: 0.8034\n",
      "Epoch 139/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3697 - accuracy: 0.8354 - val_loss: 0.5665 - val_accuracy: 0.8146\n",
      "Epoch 140/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3701 - accuracy: 0.8354 - val_loss: 0.5760 - val_accuracy: 0.8034\n",
      "Epoch 141/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3718 - accuracy: 0.8411 - val_loss: 0.5759 - val_accuracy: 0.8090\n",
      "Epoch 142/300\n",
      "711/711 [==============================] - 0s 30us/sample - loss: 0.3779 - accuracy: 0.8368 - val_loss: 0.5668 - val_accuracy: 0.7921\n",
      "Epoch 143/300\n",
      "711/711 [==============================] - 0s 31us/sample - loss: 0.3838 - accuracy: 0.8326 - val_loss: 0.5114 - val_accuracy: 0.8034\n",
      "Epoch 144/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3815 - accuracy: 0.8270 - val_loss: 0.5795 - val_accuracy: 0.7921\n",
      "Epoch 145/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.3756 - accuracy: 0.8397 - val_loss: 0.5751 - val_accuracy: 0.7978\n",
      "Epoch 146/300\n",
      "711/711 [==============================] - 0s 31us/sample - loss: 0.3710 - accuracy: 0.8368 - val_loss: 0.5637 - val_accuracy: 0.8090\n",
      "Epoch 147/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3738 - accuracy: 0.8411 - val_loss: 0.5714 - val_accuracy: 0.8090\n",
      "Epoch 148/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3681 - accuracy: 0.8397 - val_loss: 0.5682 - val_accuracy: 0.8090\n",
      "Epoch 149/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.3708 - accuracy: 0.8326 - val_loss: 0.5769 - val_accuracy: 0.7921\n",
      "Epoch 150/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3706 - accuracy: 0.8397 - val_loss: 0.5699 - val_accuracy: 0.8034\n",
      "Epoch 151/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3693 - accuracy: 0.8383 - val_loss: 0.5761 - val_accuracy: 0.8090\n",
      "Epoch 152/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3706 - accuracy: 0.8383 - val_loss: 0.5770 - val_accuracy: 0.8146\n",
      "Epoch 153/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3681 - accuracy: 0.8340 - val_loss: 0.5732 - val_accuracy: 0.8146\n",
      "Epoch 154/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3698 - accuracy: 0.8354 - val_loss: 0.5767 - val_accuracy: 0.8146\n",
      "Epoch 155/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3707 - accuracy: 0.8368 - val_loss: 0.5737 - val_accuracy: 0.8034\n",
      "Epoch 156/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3696 - accuracy: 0.8411 - val_loss: 0.5770 - val_accuracy: 0.7978\n",
      "Epoch 157/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3756 - accuracy: 0.8326 - val_loss: 0.5718 - val_accuracy: 0.8034\n",
      "Epoch 158/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3865 - accuracy: 0.8354 - val_loss: 0.5089 - val_accuracy: 0.8146\n",
      "Epoch 159/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3832 - accuracy: 0.8284 - val_loss: 0.5681 - val_accuracy: 0.8146\n",
      "Epoch 160/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3787 - accuracy: 0.8368 - val_loss: 0.5813 - val_accuracy: 0.7978\n",
      "Epoch 161/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3743 - accuracy: 0.8397 - val_loss: 0.5796 - val_accuracy: 0.8090\n",
      "Epoch 162/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3706 - accuracy: 0.8354 - val_loss: 0.5696 - val_accuracy: 0.7978\n",
      "Epoch 163/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3709 - accuracy: 0.8368 - val_loss: 0.5715 - val_accuracy: 0.8034\n",
      "Epoch 164/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3700 - accuracy: 0.8397 - val_loss: 0.5781 - val_accuracy: 0.8146\n",
      "Epoch 165/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3695 - accuracy: 0.8411 - val_loss: 0.5713 - val_accuracy: 0.8034\n",
      "Epoch 166/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3695 - accuracy: 0.8340 - val_loss: 0.5725 - val_accuracy: 0.8090\n",
      "Epoch 167/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3696 - accuracy: 0.8411 - val_loss: 0.5799 - val_accuracy: 0.8090\n",
      "Epoch 168/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3718 - accuracy: 0.8298 - val_loss: 0.5729 - val_accuracy: 0.8146\n",
      "Epoch 169/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3676 - accuracy: 0.8383 - val_loss: 0.5280 - val_accuracy: 0.7978\n",
      "Epoch 170/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3688 - accuracy: 0.8383 - val_loss: 0.5784 - val_accuracy: 0.8034\n",
      "Epoch 171/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3698 - accuracy: 0.8354 - val_loss: 0.5757 - val_accuracy: 0.8034\n",
      "Epoch 172/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3705 - accuracy: 0.8425 - val_loss: 0.5837 - val_accuracy: 0.8034\n",
      "Epoch 173/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3691 - accuracy: 0.8383 - val_loss: 0.5813 - val_accuracy: 0.8090\n",
      "Epoch 174/300\n",
      "711/711 [==============================] - 0s 26us/sample - loss: 0.3708 - accuracy: 0.8397 - val_loss: 0.5350 - val_accuracy: 0.7978\n",
      "Epoch 175/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3739 - accuracy: 0.8383 - val_loss: 0.5737 - val_accuracy: 0.8146\n",
      "Epoch 176/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3688 - accuracy: 0.8383 - val_loss: 0.5809 - val_accuracy: 0.7978\n",
      "Epoch 177/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3669 - accuracy: 0.8383 - val_loss: 0.5770 - val_accuracy: 0.8034\n",
      "Epoch 178/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3674 - accuracy: 0.8439 - val_loss: 0.5768 - val_accuracy: 0.7978\n",
      "Epoch 179/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3685 - accuracy: 0.8411 - val_loss: 0.5770 - val_accuracy: 0.8090\n",
      "Epoch 180/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3724 - accuracy: 0.8368 - val_loss: 0.5777 - val_accuracy: 0.8034\n",
      "Epoch 181/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.3715 - accuracy: 0.8397 - val_loss: 0.5717 - val_accuracy: 0.8034\n",
      "Epoch 182/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3697 - accuracy: 0.8397 - val_loss: 0.5761 - val_accuracy: 0.8090\n",
      "Epoch 183/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3677 - accuracy: 0.8368 - val_loss: 0.5785 - val_accuracy: 0.8090\n",
      "Epoch 184/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3699 - accuracy: 0.8411 - val_loss: 0.5774 - val_accuracy: 0.7978\n",
      "Epoch 185/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3678 - accuracy: 0.8326 - val_loss: 0.5786 - val_accuracy: 0.8146\n",
      "Epoch 186/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3676 - accuracy: 0.8439 - val_loss: 0.5798 - val_accuracy: 0.8090\n",
      "Epoch 187/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3686 - accuracy: 0.8326 - val_loss: 0.5810 - val_accuracy: 0.8146\n",
      "Epoch 188/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3671 - accuracy: 0.8397 - val_loss: 0.5794 - val_accuracy: 0.7978\n",
      "Epoch 189/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3673 - accuracy: 0.8397 - val_loss: 0.5789 - val_accuracy: 0.7978\n",
      "Epoch 190/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3669 - accuracy: 0.8397 - val_loss: 0.5786 - val_accuracy: 0.8090\n",
      "Epoch 191/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3697 - accuracy: 0.8383 - val_loss: 0.5784 - val_accuracy: 0.8146\n",
      "Epoch 192/300\n",
      "711/711 [==============================] - 0s 34us/sample - loss: 0.3664 - accuracy: 0.8411 - val_loss: 0.5795 - val_accuracy: 0.8146\n",
      "Epoch 193/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3702 - accuracy: 0.8368 - val_loss: 0.5820 - val_accuracy: 0.8146\n",
      "Epoch 194/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3693 - accuracy: 0.8340 - val_loss: 0.5839 - val_accuracy: 0.8146\n",
      "Epoch 195/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3696 - accuracy: 0.8354 - val_loss: 0.5880 - val_accuracy: 0.7921\n",
      "Epoch 196/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3711 - accuracy: 0.8284 - val_loss: 0.5886 - val_accuracy: 0.7921\n",
      "Epoch 197/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3693 - accuracy: 0.8383 - val_loss: 0.5803 - val_accuracy: 0.7978\n",
      "Epoch 198/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3725 - accuracy: 0.8397 - val_loss: 0.5839 - val_accuracy: 0.8146\n",
      "Epoch 199/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3682 - accuracy: 0.8411 - val_loss: 0.5869 - val_accuracy: 0.7978\n",
      "Epoch 200/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3682 - accuracy: 0.8425 - val_loss: 0.5807 - val_accuracy: 0.8090\n",
      "Epoch 201/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3664 - accuracy: 0.8397 - val_loss: 0.5818 - val_accuracy: 0.8146\n",
      "Epoch 202/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3672 - accuracy: 0.8425 - val_loss: 0.5212 - val_accuracy: 0.8034\n",
      "Epoch 203/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3675 - accuracy: 0.8383 - val_loss: 0.5867 - val_accuracy: 0.8034\n",
      "Epoch 204/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3668 - accuracy: 0.8425 - val_loss: 0.5851 - val_accuracy: 0.7978\n",
      "Epoch 205/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3652 - accuracy: 0.8354 - val_loss: 0.5796 - val_accuracy: 0.8034\n",
      "Epoch 206/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3670 - accuracy: 0.8368 - val_loss: 0.5914 - val_accuracy: 0.8090\n",
      "Epoch 207/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3655 - accuracy: 0.8383 - val_loss: 0.5895 - val_accuracy: 0.8146\n",
      "Epoch 208/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3662 - accuracy: 0.8383 - val_loss: 0.5891 - val_accuracy: 0.8034\n",
      "Epoch 209/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3680 - accuracy: 0.8397 - val_loss: 0.5815 - val_accuracy: 0.8090\n",
      "Epoch 210/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3662 - accuracy: 0.8368 - val_loss: 0.5893 - val_accuracy: 0.8090\n",
      "Epoch 211/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3658 - accuracy: 0.8397 - val_loss: 0.5883 - val_accuracy: 0.8090\n",
      "Epoch 212/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3665 - accuracy: 0.8368 - val_loss: 0.5864 - val_accuracy: 0.7978\n",
      "Epoch 213/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3680 - accuracy: 0.8383 - val_loss: 0.6562 - val_accuracy: 0.8034\n",
      "Epoch 214/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3671 - accuracy: 0.8368 - val_loss: 0.5934 - val_accuracy: 0.7978\n",
      "Epoch 215/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3667 - accuracy: 0.8368 - val_loss: 0.5859 - val_accuracy: 0.8090\n",
      "Epoch 216/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3649 - accuracy: 0.8411 - val_loss: 0.5918 - val_accuracy: 0.8090\n",
      "Epoch 217/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3672 - accuracy: 0.8425 - val_loss: 0.6073 - val_accuracy: 0.8034\n",
      "Epoch 218/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3671 - accuracy: 0.8397 - val_loss: 0.5887 - val_accuracy: 0.8034\n",
      "Epoch 219/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3681 - accuracy: 0.8312 - val_loss: 0.5887 - val_accuracy: 0.8034\n",
      "Epoch 220/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3692 - accuracy: 0.8383 - val_loss: 0.6030 - val_accuracy: 0.8146\n",
      "Epoch 221/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3666 - accuracy: 0.8368 - val_loss: 0.5930 - val_accuracy: 0.8146\n",
      "Epoch 222/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3697 - accuracy: 0.8411 - val_loss: 0.6516 - val_accuracy: 0.8146\n",
      "Epoch 223/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3692 - accuracy: 0.8368 - val_loss: 0.5945 - val_accuracy: 0.8034\n",
      "Epoch 224/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3673 - accuracy: 0.8383 - val_loss: 0.5954 - val_accuracy: 0.7978\n",
      "Epoch 225/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3657 - accuracy: 0.8397 - val_loss: 0.6578 - val_accuracy: 0.8034\n",
      "Epoch 226/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3674 - accuracy: 0.8397 - val_loss: 0.6503 - val_accuracy: 0.8034\n",
      "Epoch 227/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3693 - accuracy: 0.8368 - val_loss: 0.6536 - val_accuracy: 0.8146\n",
      "Epoch 228/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3685 - accuracy: 0.8425 - val_loss: 0.6094 - val_accuracy: 0.8090\n",
      "Epoch 229/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3693 - accuracy: 0.8354 - val_loss: 0.6075 - val_accuracy: 0.8034\n",
      "Epoch 230/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3687 - accuracy: 0.8383 - val_loss: 0.6553 - val_accuracy: 0.8146\n",
      "Epoch 231/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3705 - accuracy: 0.8425 - val_loss: 0.6558 - val_accuracy: 0.8090\n",
      "Epoch 232/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3661 - accuracy: 0.8425 - val_loss: 0.6510 - val_accuracy: 0.8034\n",
      "Epoch 233/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3655 - accuracy: 0.8411 - val_loss: 0.5911 - val_accuracy: 0.8034\n",
      "Epoch 234/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3663 - accuracy: 0.8397 - val_loss: 0.6554 - val_accuracy: 0.8146\n",
      "Epoch 235/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3667 - accuracy: 0.8368 - val_loss: 0.6572 - val_accuracy: 0.8146\n",
      "Epoch 236/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3666 - accuracy: 0.8383 - val_loss: 0.6536 - val_accuracy: 0.8090\n",
      "Epoch 237/300\n",
      "711/711 [==============================] - 0s 23us/sample - loss: 0.3659 - accuracy: 0.8368 - val_loss: 0.6032 - val_accuracy: 0.8090\n",
      "Epoch 238/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3657 - accuracy: 0.8368 - val_loss: 0.5973 - val_accuracy: 0.8146\n",
      "Epoch 239/300\n",
      "711/711 [==============================] - ETA: 0s - loss: 0.4629 - accuracy: 0.83 - 0s 21us/sample - loss: 0.3662 - accuracy: 0.8368 - val_loss: 0.6522 - val_accuracy: 0.8034\n",
      "Epoch 240/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3657 - accuracy: 0.8354 - val_loss: 0.6557 - val_accuracy: 0.8090\n",
      "Epoch 241/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3647 - accuracy: 0.8425 - val_loss: 0.6641 - val_accuracy: 0.8034\n",
      "Epoch 242/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3660 - accuracy: 0.8397 - val_loss: 0.6573 - val_accuracy: 0.8090\n",
      "Epoch 243/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3645 - accuracy: 0.8411 - val_loss: 0.6554 - val_accuracy: 0.8090\n",
      "Epoch 244/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3645 - accuracy: 0.8397 - val_loss: 0.6573 - val_accuracy: 0.8146\n",
      "Epoch 245/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3655 - accuracy: 0.8368 - val_loss: 0.6568 - val_accuracy: 0.8146\n",
      "Epoch 246/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3655 - accuracy: 0.8368 - val_loss: 0.6633 - val_accuracy: 0.8034\n",
      "Epoch 247/300\n",
      "711/711 [==============================] - 0s 23us/sample - loss: 0.3654 - accuracy: 0.8383 - val_loss: 0.5887 - val_accuracy: 0.8146\n",
      "Epoch 248/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3657 - accuracy: 0.8368 - val_loss: 0.6063 - val_accuracy: 0.8034\n",
      "Epoch 249/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3655 - accuracy: 0.8383 - val_loss: 0.6568 - val_accuracy: 0.8090\n",
      "Epoch 250/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3651 - accuracy: 0.8326 - val_loss: 0.5960 - val_accuracy: 0.7978\n",
      "Epoch 251/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3659 - accuracy: 0.8383 - val_loss: 0.6019 - val_accuracy: 0.8090\n",
      "Epoch 252/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3661 - accuracy: 0.8383 - val_loss: 0.6582 - val_accuracy: 0.8090\n",
      "Epoch 253/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3656 - accuracy: 0.8354 - val_loss: 0.6596 - val_accuracy: 0.8146\n",
      "Epoch 254/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3647 - accuracy: 0.8411 - val_loss: 0.6615 - val_accuracy: 0.8090\n",
      "Epoch 255/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3638 - accuracy: 0.8383 - val_loss: 0.5936 - val_accuracy: 0.8034\n",
      "Epoch 256/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3685 - accuracy: 0.8383 - val_loss: 0.6542 - val_accuracy: 0.7978\n",
      "Epoch 257/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3666 - accuracy: 0.8397 - val_loss: 0.5967 - val_accuracy: 0.8090\n",
      "Epoch 258/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3683 - accuracy: 0.8383 - val_loss: 0.6016 - val_accuracy: 0.8146\n",
      "Epoch 259/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3662 - accuracy: 0.8368 - val_loss: 0.6646 - val_accuracy: 0.8090\n",
      "Epoch 260/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3682 - accuracy: 0.8340 - val_loss: 0.6565 - val_accuracy: 0.7921\n",
      "Epoch 261/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3668 - accuracy: 0.8397 - val_loss: 0.6577 - val_accuracy: 0.8090\n",
      "Epoch 262/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3678 - accuracy: 0.8383 - val_loss: 0.6009 - val_accuracy: 0.8090\n",
      "Epoch 263/300\n",
      "711/711 [==============================] - 0s 23us/sample - loss: 0.3697 - accuracy: 0.8312 - val_loss: 0.6656 - val_accuracy: 0.8146\n",
      "Epoch 264/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3693 - accuracy: 0.8411 - val_loss: 0.6517 - val_accuracy: 0.8034\n",
      "Epoch 265/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3661 - accuracy: 0.8383 - val_loss: 0.5867 - val_accuracy: 0.8146\n",
      "Epoch 266/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3672 - accuracy: 0.8368 - val_loss: 0.6566 - val_accuracy: 0.8090\n",
      "Epoch 267/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3689 - accuracy: 0.8312 - val_loss: 0.6613 - val_accuracy: 0.8146\n",
      "Epoch 268/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3657 - accuracy: 0.8397 - val_loss: 0.6575 - val_accuracy: 0.7978\n",
      "Epoch 269/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3675 - accuracy: 0.8411 - val_loss: 0.5936 - val_accuracy: 0.8034\n",
      "Epoch 270/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3660 - accuracy: 0.8383 - val_loss: 0.6592 - val_accuracy: 0.8146\n",
      "Epoch 271/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3657 - accuracy: 0.8397 - val_loss: 0.6550 - val_accuracy: 0.8034\n",
      "Epoch 272/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3655 - accuracy: 0.8354 - val_loss: 0.6506 - val_accuracy: 0.7865\n",
      "Epoch 273/300\n",
      "711/711 [==============================] - 0s 23us/sample - loss: 0.3655 - accuracy: 0.8383 - val_loss: 0.6589 - val_accuracy: 0.8090\n",
      "Epoch 274/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3662 - accuracy: 0.8425 - val_loss: 0.6094 - val_accuracy: 0.8146\n",
      "Epoch 275/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3655 - accuracy: 0.8383 - val_loss: 0.6497 - val_accuracy: 0.8034\n",
      "Epoch 276/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3675 - accuracy: 0.8340 - val_loss: 0.5866 - val_accuracy: 0.8034\n",
      "Epoch 277/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3645 - accuracy: 0.8340 - val_loss: 0.6622 - val_accuracy: 0.8034\n",
      "Epoch 278/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3663 - accuracy: 0.8383 - val_loss: 0.6590 - val_accuracy: 0.8034\n",
      "Epoch 279/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3677 - accuracy: 0.8383 - val_loss: 0.6573 - val_accuracy: 0.8034\n",
      "Epoch 280/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3662 - accuracy: 0.8368 - val_loss: 0.6020 - val_accuracy: 0.8146\n",
      "Epoch 281/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3659 - accuracy: 0.8354 - val_loss: 0.6033 - val_accuracy: 0.8034\n",
      "Epoch 282/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3658 - accuracy: 0.8368 - val_loss: 0.6537 - val_accuracy: 0.8034\n",
      "Epoch 283/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3636 - accuracy: 0.8340 - val_loss: 0.6574 - val_accuracy: 0.8090\n",
      "Epoch 284/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3695 - accuracy: 0.8340 - val_loss: 0.6034 - val_accuracy: 0.8034\n",
      "Epoch 285/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3667 - accuracy: 0.8368 - val_loss: 0.6576 - val_accuracy: 0.8034\n",
      "Epoch 286/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3644 - accuracy: 0.8397 - val_loss: 0.6577 - val_accuracy: 0.8146\n",
      "Epoch 287/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3653 - accuracy: 0.8397 - val_loss: 0.6605 - val_accuracy: 0.8146\n",
      "Epoch 288/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3667 - accuracy: 0.8397 - val_loss: 0.6623 - val_accuracy: 0.8034\n",
      "Epoch 289/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3658 - accuracy: 0.8340 - val_loss: 0.6040 - val_accuracy: 0.8146\n",
      "Epoch 290/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3629 - accuracy: 0.8411 - val_loss: 0.6640 - val_accuracy: 0.8034\n",
      "Epoch 291/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3643 - accuracy: 0.8354 - val_loss: 0.6650 - val_accuracy: 0.8146\n",
      "Epoch 292/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3647 - accuracy: 0.8368 - val_loss: 0.6540 - val_accuracy: 0.8034\n",
      "Epoch 293/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3637 - accuracy: 0.8368 - val_loss: 0.6551 - val_accuracy: 0.8146\n",
      "Epoch 294/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3638 - accuracy: 0.8383 - val_loss: 0.5949 - val_accuracy: 0.8146\n",
      "Epoch 295/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3632 - accuracy: 0.8411 - val_loss: 0.6556 - val_accuracy: 0.8034\n",
      "Epoch 296/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3643 - accuracy: 0.8340 - val_loss: 0.6582 - val_accuracy: 0.8034\n",
      "Epoch 297/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3641 - accuracy: 0.8354 - val_loss: 0.6121 - val_accuracy: 0.8034\n",
      "Epoch 298/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3658 - accuracy: 0.8354 - val_loss: 0.6562 - val_accuracy: 0.8034\n",
      "Epoch 299/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3640 - accuracy: 0.8397 - val_loss: 0.6642 - val_accuracy: 0.8034\n",
      "Epoch 300/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3677 - accuracy: 0.8368 - val_loss: 0.6627 - val_accuracy: 0.7978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21fe0ff7c08>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"model.fit(\\n    X_train,\\n    y_train,\\n    validation_data=(X_test, y_test),\\n    batch_size=80,\\n    epochs=300,\\n    verbose=1,\\n)\";\n",
       "                var nbb_formatted_code = \"model.fit(\\n    X_train,\\n    y_train,\\n    validation_data=(X_test, y_test),\\n    batch_size=80,\\n    epochs=300,\\n    verbose=1,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=80,\n",
    "    epochs=300,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zsDsEJ0b0jTV"
   },
   "source": [
    "Redefine the model using a sigmoid activation for the last layer. What is the difference in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsbGU_gM0jTV"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# Answer below\\nmodel2 = Sequential()\\nmodel2.add(Dense(64, input_dim=X_train.shape[1], activation=\\\"relu\\\"))\\nmodel2.add(Dense(32, activation=\\\"relu\\\"))\\nmodel2.add(Dense(16, activation=\\\"relu\\\"))\\nmodel2.add(Dense(8, activation=\\\"relu\\\"))\\nmodel2.add(Dense(1, activation=\\\"sigmoid\\\"))\";\n",
       "                var nbb_formatted_code = \"# Answer below\\nmodel2 = Sequential()\\nmodel2.add(Dense(64, input_dim=X_train.shape[1], activation=\\\"relu\\\"))\\nmodel2.add(Dense(32, activation=\\\"relu\\\"))\\nmodel2.add(Dense(16, activation=\\\"relu\\\"))\\nmodel2.add(Dense(8, activation=\\\"relu\\\"))\\nmodel2.add(Dense(1, activation=\\\"sigmoid\\\"))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(64, input_dim=X_train.shape[1], activation=\"relu\"))\n",
    "model2.add(Dense(32, activation=\"relu\"))\n",
    "model2.add(Dense(16, activation=\"relu\"))\n",
    "model2.add(Dense(8, activation=\"relu\"))\n",
    "model2.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m8BpyzL40jTX"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"model2.compile(optimizer=\\\"adam\\\", loss=\\\"binary_crossentropy\\\", metrics=[\\\"accuracy\\\"])\";\n",
       "                var nbb_formatted_code = \"model2.compile(optimizer=\\\"adam\\\", loss=\\\"binary_crossentropy\\\", metrics=[\\\"accuracy\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 711 samples, validate on 178 samples\n",
      "Epoch 1/300\n",
      "711/711 [==============================] - 0s 167us/sample - loss: 0.6869 - accuracy: 0.5879 - val_loss: 0.6763 - val_accuracy: 0.6067\n",
      "Epoch 2/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.6667 - accuracy: 0.6526 - val_loss: 0.6587 - val_accuracy: 0.6011\n",
      "Epoch 3/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.6467 - accuracy: 0.6245 - val_loss: 0.6383 - val_accuracy: 0.5899\n",
      "Epoch 4/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.6242 - accuracy: 0.6301 - val_loss: 0.6186 - val_accuracy: 0.6348\n",
      "Epoch 5/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.6007 - accuracy: 0.6821 - val_loss: 0.5986 - val_accuracy: 0.6742\n",
      "Epoch 6/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.5779 - accuracy: 0.7173 - val_loss: 0.5819 - val_accuracy: 0.6742\n",
      "Epoch 7/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.5550 - accuracy: 0.7398 - val_loss: 0.5634 - val_accuracy: 0.7416\n",
      "Epoch 8/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.5333 - accuracy: 0.7820 - val_loss: 0.5440 - val_accuracy: 0.7528\n",
      "Epoch 9/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.5128 - accuracy: 0.7890 - val_loss: 0.5207 - val_accuracy: 0.7697\n",
      "Epoch 10/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.4962 - accuracy: 0.7848 - val_loss: 0.5042 - val_accuracy: 0.7697\n",
      "Epoch 11/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.4812 - accuracy: 0.7904 - val_loss: 0.4862 - val_accuracy: 0.7921\n",
      "Epoch 12/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.4697 - accuracy: 0.7989 - val_loss: 0.4708 - val_accuracy: 0.7865\n",
      "Epoch 13/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.4625 - accuracy: 0.8031 - val_loss: 0.4615 - val_accuracy: 0.7921\n",
      "Epoch 14/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.4537 - accuracy: 0.8101 - val_loss: 0.4535 - val_accuracy: 0.7921\n",
      "Epoch 15/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.4518 - accuracy: 0.8073 - val_loss: 0.4501 - val_accuracy: 0.7978\n",
      "Epoch 16/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.4460 - accuracy: 0.8101 - val_loss: 0.4512 - val_accuracy: 0.8034\n",
      "Epoch 17/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.4414 - accuracy: 0.8143 - val_loss: 0.4361 - val_accuracy: 0.7865\n",
      "Epoch 18/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.4370 - accuracy: 0.8158 - val_loss: 0.4449 - val_accuracy: 0.7978\n",
      "Epoch 19/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.4342 - accuracy: 0.8158 - val_loss: 0.4354 - val_accuracy: 0.8034\n",
      "Epoch 20/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.4310 - accuracy: 0.8129 - val_loss: 0.4361 - val_accuracy: 0.8034\n",
      "Epoch 21/300\n",
      "711/711 [==============================] - 0s 34us/sample - loss: 0.4273 - accuracy: 0.8143 - val_loss: 0.4286 - val_accuracy: 0.7865\n",
      "Epoch 22/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.4262 - accuracy: 0.8101 - val_loss: 0.4300 - val_accuracy: 0.7978\n",
      "Epoch 23/300\n",
      "711/711 [==============================] - 0s 31us/sample - loss: 0.4252 - accuracy: 0.8172 - val_loss: 0.4332 - val_accuracy: 0.8034\n",
      "Epoch 24/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.4224 - accuracy: 0.8143 - val_loss: 0.4293 - val_accuracy: 0.7978\n",
      "Epoch 25/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.4205 - accuracy: 0.8143 - val_loss: 0.4278 - val_accuracy: 0.8034\n",
      "Epoch 26/300\n",
      "711/711 [==============================] - 0s 34us/sample - loss: 0.4199 - accuracy: 0.8143 - val_loss: 0.4265 - val_accuracy: 0.8034\n",
      "Epoch 27/300\n",
      "711/711 [==============================] - 0s 34us/sample - loss: 0.4196 - accuracy: 0.8129 - val_loss: 0.4277 - val_accuracy: 0.7978\n",
      "Epoch 28/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.4247 - accuracy: 0.8087 - val_loss: 0.4264 - val_accuracy: 0.8034\n",
      "Epoch 29/300\n",
      "711/711 [==============================] - 0s 36us/sample - loss: 0.4208 - accuracy: 0.8101 - val_loss: 0.4217 - val_accuracy: 0.8146\n",
      "Epoch 30/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.4144 - accuracy: 0.8200 - val_loss: 0.4304 - val_accuracy: 0.8034\n",
      "Epoch 31/300\n",
      "711/711 [==============================] - ETA: 0s - loss: 0.4961 - accuracy: 0.80 - 0s 36us/sample - loss: 0.4152 - accuracy: 0.8200 - val_loss: 0.4216 - val_accuracy: 0.8034\n",
      "Epoch 32/300\n",
      "711/711 [==============================] - 0s 34us/sample - loss: 0.4151 - accuracy: 0.8200 - val_loss: 0.4260 - val_accuracy: 0.8034\n",
      "Epoch 33/300\n",
      "711/711 [==============================] - 0s 38us/sample - loss: 0.4150 - accuracy: 0.8172 - val_loss: 0.4289 - val_accuracy: 0.8034\n",
      "Epoch 34/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.4112 - accuracy: 0.8214 - val_loss: 0.4202 - val_accuracy: 0.8146\n",
      "Epoch 35/300\n",
      "711/711 [==============================] - 0s 36us/sample - loss: 0.4109 - accuracy: 0.8200 - val_loss: 0.4246 - val_accuracy: 0.8034\n",
      "Epoch 36/300\n",
      "711/711 [==============================] - 0s 34us/sample - loss: 0.4109 - accuracy: 0.8186 - val_loss: 0.4207 - val_accuracy: 0.8034\n",
      "Epoch 37/300\n",
      "711/711 [==============================] - 0s 29us/sample - loss: 0.4095 - accuracy: 0.8214 - val_loss: 0.4222 - val_accuracy: 0.8090\n",
      "Epoch 38/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.4102 - accuracy: 0.8242 - val_loss: 0.4264 - val_accuracy: 0.7978\n",
      "Epoch 39/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.4118 - accuracy: 0.8186 - val_loss: 0.4235 - val_accuracy: 0.8034\n",
      "Epoch 40/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.4124 - accuracy: 0.8200 - val_loss: 0.4155 - val_accuracy: 0.8090\n",
      "Epoch 41/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.4088 - accuracy: 0.8242 - val_loss: 0.4304 - val_accuracy: 0.8034\n",
      "Epoch 42/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.4060 - accuracy: 0.8242 - val_loss: 0.4191 - val_accuracy: 0.8090\n",
      "Epoch 43/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.4069 - accuracy: 0.8228 - val_loss: 0.4190 - val_accuracy: 0.8146\n",
      "Epoch 44/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.4120 - accuracy: 0.8214 - val_loss: 0.4250 - val_accuracy: 0.8034\n",
      "Epoch 45/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.4101 - accuracy: 0.8256 - val_loss: 0.4170 - val_accuracy: 0.8202\n",
      "Epoch 46/300\n",
      "711/711 [==============================] - 0s 34us/sample - loss: 0.4038 - accuracy: 0.8256 - val_loss: 0.4302 - val_accuracy: 0.7978\n",
      "Epoch 47/300\n",
      "711/711 [==============================] - 0s 38us/sample - loss: 0.4053 - accuracy: 0.8228 - val_loss: 0.4216 - val_accuracy: 0.8146\n",
      "Epoch 48/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.4071 - accuracy: 0.8228 - val_loss: 0.4199 - val_accuracy: 0.8146\n",
      "Epoch 49/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.4031 - accuracy: 0.8256 - val_loss: 0.4277 - val_accuracy: 0.7978\n",
      "Epoch 50/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.4038 - accuracy: 0.8256 - val_loss: 0.4183 - val_accuracy: 0.8090\n",
      "Epoch 51/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.4036 - accuracy: 0.8242 - val_loss: 0.4200 - val_accuracy: 0.8202\n",
      "Epoch 52/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.4016 - accuracy: 0.8228 - val_loss: 0.4270 - val_accuracy: 0.8090\n",
      "Epoch 53/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.4031 - accuracy: 0.8242 - val_loss: 0.4185 - val_accuracy: 0.8202\n",
      "Epoch 54/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.4002 - accuracy: 0.8228 - val_loss: 0.4163 - val_accuracy: 0.8202\n",
      "Epoch 55/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711/711 [==============================] - 0s 22us/sample - loss: 0.4029 - accuracy: 0.8228 - val_loss: 0.4200 - val_accuracy: 0.8146\n",
      "Epoch 56/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.4004 - accuracy: 0.8256 - val_loss: 0.4207 - val_accuracy: 0.8090\n",
      "Epoch 57/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.4022 - accuracy: 0.8256 - val_loss: 0.4250 - val_accuracy: 0.8090\n",
      "Epoch 58/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.4001 - accuracy: 0.8284 - val_loss: 0.4182 - val_accuracy: 0.8202\n",
      "Epoch 59/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.4014 - accuracy: 0.8284 - val_loss: 0.4262 - val_accuracy: 0.8090\n",
      "Epoch 60/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3993 - accuracy: 0.8270 - val_loss: 0.4181 - val_accuracy: 0.8090\n",
      "Epoch 61/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3990 - accuracy: 0.8270 - val_loss: 0.4215 - val_accuracy: 0.8090\n",
      "Epoch 62/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3988 - accuracy: 0.8270 - val_loss: 0.4192 - val_accuracy: 0.8090\n",
      "Epoch 63/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3981 - accuracy: 0.8256 - val_loss: 0.4207 - val_accuracy: 0.8202\n",
      "Epoch 64/300\n",
      "711/711 [==============================] - 0s 23us/sample - loss: 0.3979 - accuracy: 0.8270 - val_loss: 0.4185 - val_accuracy: 0.8090\n",
      "Epoch 65/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3974 - accuracy: 0.8256 - val_loss: 0.4209 - val_accuracy: 0.8090\n",
      "Epoch 66/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3997 - accuracy: 0.8298 - val_loss: 0.4204 - val_accuracy: 0.8146\n",
      "Epoch 67/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3979 - accuracy: 0.8242 - val_loss: 0.4278 - val_accuracy: 0.8034\n",
      "Epoch 68/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3969 - accuracy: 0.8270 - val_loss: 0.4177 - val_accuracy: 0.8146\n",
      "Epoch 69/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.4007 - accuracy: 0.8256 - val_loss: 0.4240 - val_accuracy: 0.8090\n",
      "Epoch 70/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3968 - accuracy: 0.8256 - val_loss: 0.4171 - val_accuracy: 0.8146\n",
      "Epoch 71/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3988 - accuracy: 0.8256 - val_loss: 0.4216 - val_accuracy: 0.8090\n",
      "Epoch 72/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3950 - accuracy: 0.8298 - val_loss: 0.4205 - val_accuracy: 0.8202\n",
      "Epoch 73/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.4002 - accuracy: 0.8228 - val_loss: 0.4228 - val_accuracy: 0.8146\n",
      "Epoch 74/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3996 - accuracy: 0.8242 - val_loss: 0.4176 - val_accuracy: 0.8090\n",
      "Epoch 75/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3976 - accuracy: 0.8270 - val_loss: 0.4225 - val_accuracy: 0.8090\n",
      "Epoch 76/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3955 - accuracy: 0.8256 - val_loss: 0.4180 - val_accuracy: 0.8258\n",
      "Epoch 77/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3946 - accuracy: 0.8284 - val_loss: 0.4227 - val_accuracy: 0.8090\n",
      "Epoch 78/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3945 - accuracy: 0.8298 - val_loss: 0.4184 - val_accuracy: 0.8146\n",
      "Epoch 79/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3938 - accuracy: 0.8256 - val_loss: 0.4261 - val_accuracy: 0.8090\n",
      "Epoch 80/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3943 - accuracy: 0.8270 - val_loss: 0.4180 - val_accuracy: 0.8090\n",
      "Epoch 81/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3971 - accuracy: 0.8228 - val_loss: 0.4186 - val_accuracy: 0.8090\n",
      "Epoch 82/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.4030 - accuracy: 0.8270 - val_loss: 0.4275 - val_accuracy: 0.8034\n",
      "Epoch 83/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3927 - accuracy: 0.8284 - val_loss: 0.4182 - val_accuracy: 0.8202\n",
      "Epoch 84/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3940 - accuracy: 0.8298 - val_loss: 0.4222 - val_accuracy: 0.8146\n",
      "Epoch 85/300\n",
      "711/711 [==============================] - 0s 26us/sample - loss: 0.3941 - accuracy: 0.8298 - val_loss: 0.4257 - val_accuracy: 0.8034\n",
      "Epoch 86/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3936 - accuracy: 0.8298 - val_loss: 0.4160 - val_accuracy: 0.8202\n",
      "Epoch 87/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3940 - accuracy: 0.8284 - val_loss: 0.4242 - val_accuracy: 0.8090\n",
      "Epoch 88/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3951 - accuracy: 0.8284 - val_loss: 0.4231 - val_accuracy: 0.8090\n",
      "Epoch 89/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3975 - accuracy: 0.8284 - val_loss: 0.4195 - val_accuracy: 0.8202\n",
      "Epoch 90/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3944 - accuracy: 0.8242 - val_loss: 0.4271 - val_accuracy: 0.7978\n",
      "Epoch 91/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3929 - accuracy: 0.8284 - val_loss: 0.4156 - val_accuracy: 0.8146\n",
      "Epoch 92/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3923 - accuracy: 0.8270 - val_loss: 0.4271 - val_accuracy: 0.8034\n",
      "Epoch 93/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3907 - accuracy: 0.8270 - val_loss: 0.4200 - val_accuracy: 0.8090\n",
      "Epoch 94/300\n",
      "711/711 [==============================] - 0s 31us/sample - loss: 0.3904 - accuracy: 0.8298 - val_loss: 0.4176 - val_accuracy: 0.8090\n",
      "Epoch 95/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3909 - accuracy: 0.8242 - val_loss: 0.4272 - val_accuracy: 0.8034\n",
      "Epoch 96/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3906 - accuracy: 0.8312 - val_loss: 0.4203 - val_accuracy: 0.8090\n",
      "Epoch 97/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3910 - accuracy: 0.8298 - val_loss: 0.4226 - val_accuracy: 0.8090\n",
      "Epoch 98/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3903 - accuracy: 0.8298 - val_loss: 0.4256 - val_accuracy: 0.8090\n",
      "Epoch 99/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3921 - accuracy: 0.8284 - val_loss: 0.4228 - val_accuracy: 0.8090\n",
      "Epoch 100/300\n",
      "711/711 [==============================] - 0s 23us/sample - loss: 0.3924 - accuracy: 0.8270 - val_loss: 0.4179 - val_accuracy: 0.8090\n",
      "Epoch 101/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3898 - accuracy: 0.8284 - val_loss: 0.4216 - val_accuracy: 0.8090\n",
      "Epoch 102/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3904 - accuracy: 0.8298 - val_loss: 0.4221 - val_accuracy: 0.8090\n",
      "Epoch 103/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3897 - accuracy: 0.8298 - val_loss: 0.4272 - val_accuracy: 0.8090\n",
      "Epoch 104/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3947 - accuracy: 0.8354 - val_loss: 0.4207 - val_accuracy: 0.8090\n",
      "Epoch 105/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3930 - accuracy: 0.8270 - val_loss: 0.4258 - val_accuracy: 0.8034\n",
      "Epoch 106/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3940 - accuracy: 0.8214 - val_loss: 0.4232 - val_accuracy: 0.8090\n",
      "Epoch 107/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3963 - accuracy: 0.8340 - val_loss: 0.4234 - val_accuracy: 0.8090\n",
      "Epoch 108/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3930 - accuracy: 0.8326 - val_loss: 0.4199 - val_accuracy: 0.8202\n",
      "Epoch 109/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3883 - accuracy: 0.8383 - val_loss: 0.4255 - val_accuracy: 0.7978\n",
      "Epoch 110/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3914 - accuracy: 0.8298 - val_loss: 0.4290 - val_accuracy: 0.8090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.3906 - accuracy: 0.8312 - val_loss: 0.4207 - val_accuracy: 0.8090\n",
      "Epoch 112/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.3902 - accuracy: 0.8284 - val_loss: 0.4179 - val_accuracy: 0.8090\n",
      "Epoch 113/300\n",
      "711/711 [==============================] - 0s 32us/sample - loss: 0.3926 - accuracy: 0.8270 - val_loss: 0.4291 - val_accuracy: 0.8090\n",
      "Epoch 114/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3879 - accuracy: 0.8312 - val_loss: 0.4222 - val_accuracy: 0.8090\n",
      "Epoch 115/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3905 - accuracy: 0.8354 - val_loss: 0.4190 - val_accuracy: 0.8034\n",
      "Epoch 116/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3888 - accuracy: 0.8270 - val_loss: 0.4273 - val_accuracy: 0.8034\n",
      "Epoch 117/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3883 - accuracy: 0.8270 - val_loss: 0.4228 - val_accuracy: 0.8090\n",
      "Epoch 118/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3888 - accuracy: 0.8312 - val_loss: 0.4209 - val_accuracy: 0.8034\n",
      "Epoch 119/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3862 - accuracy: 0.8326 - val_loss: 0.4268 - val_accuracy: 0.8090\n",
      "Epoch 120/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3877 - accuracy: 0.8326 - val_loss: 0.4264 - val_accuracy: 0.8034\n",
      "Epoch 121/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3859 - accuracy: 0.8298 - val_loss: 0.4230 - val_accuracy: 0.8090\n",
      "Epoch 122/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3859 - accuracy: 0.8326 - val_loss: 0.4245 - val_accuracy: 0.8034\n",
      "Epoch 123/300\n",
      "711/711 [==============================] - 0s 23us/sample - loss: 0.3856 - accuracy: 0.8326 - val_loss: 0.4294 - val_accuracy: 0.8034\n",
      "Epoch 124/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3872 - accuracy: 0.8326 - val_loss: 0.4220 - val_accuracy: 0.8090\n",
      "Epoch 125/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3850 - accuracy: 0.8270 - val_loss: 0.4243 - val_accuracy: 0.8034\n",
      "Epoch 126/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3856 - accuracy: 0.8326 - val_loss: 0.4272 - val_accuracy: 0.8090\n",
      "Epoch 127/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3856 - accuracy: 0.8326 - val_loss: 0.4260 - val_accuracy: 0.8034\n",
      "Epoch 128/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3853 - accuracy: 0.8312 - val_loss: 0.4250 - val_accuracy: 0.8090\n",
      "Epoch 129/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3850 - accuracy: 0.8312 - val_loss: 0.4321 - val_accuracy: 0.7978\n",
      "Epoch 130/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3875 - accuracy: 0.8284 - val_loss: 0.4224 - val_accuracy: 0.8090\n",
      "Epoch 131/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3855 - accuracy: 0.8312 - val_loss: 0.4240 - val_accuracy: 0.8034\n",
      "Epoch 132/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3843 - accuracy: 0.8326 - val_loss: 0.4258 - val_accuracy: 0.8034\n",
      "Epoch 133/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3850 - accuracy: 0.8298 - val_loss: 0.4276 - val_accuracy: 0.8034\n",
      "Epoch 134/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3839 - accuracy: 0.8284 - val_loss: 0.4223 - val_accuracy: 0.8090\n",
      "Epoch 135/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3848 - accuracy: 0.8326 - val_loss: 0.4266 - val_accuracy: 0.8090\n",
      "Epoch 136/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3829 - accuracy: 0.8326 - val_loss: 0.4262 - val_accuracy: 0.8090\n",
      "Epoch 137/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3835 - accuracy: 0.8326 - val_loss: 0.4265 - val_accuracy: 0.8090\n",
      "Epoch 138/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3826 - accuracy: 0.8326 - val_loss: 0.4280 - val_accuracy: 0.8034\n",
      "Epoch 139/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3829 - accuracy: 0.8326 - val_loss: 0.4278 - val_accuracy: 0.8090\n",
      "Epoch 140/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3831 - accuracy: 0.8312 - val_loss: 0.4291 - val_accuracy: 0.8090\n",
      "Epoch 141/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3856 - accuracy: 0.8326 - val_loss: 0.4227 - val_accuracy: 0.8090\n",
      "Epoch 142/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3875 - accuracy: 0.8312 - val_loss: 0.4315 - val_accuracy: 0.8034\n",
      "Epoch 143/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3841 - accuracy: 0.8326 - val_loss: 0.4251 - val_accuracy: 0.8090\n",
      "Epoch 144/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3816 - accuracy: 0.8326 - val_loss: 0.4252 - val_accuracy: 0.8090\n",
      "Epoch 145/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3845 - accuracy: 0.8284 - val_loss: 0.4276 - val_accuracy: 0.8090\n",
      "Epoch 146/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3880 - accuracy: 0.8256 - val_loss: 0.4299 - val_accuracy: 0.8090\n",
      "Epoch 147/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3934 - accuracy: 0.8298 - val_loss: 0.4289 - val_accuracy: 0.8034\n",
      "Epoch 148/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3868 - accuracy: 0.8326 - val_loss: 0.4292 - val_accuracy: 0.8090\n",
      "Epoch 149/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3857 - accuracy: 0.8326 - val_loss: 0.4294 - val_accuracy: 0.7978\n",
      "Epoch 150/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3810 - accuracy: 0.8326 - val_loss: 0.4253 - val_accuracy: 0.8146\n",
      "Epoch 151/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3832 - accuracy: 0.8312 - val_loss: 0.4314 - val_accuracy: 0.8034\n",
      "Epoch 152/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3821 - accuracy: 0.8340 - val_loss: 0.4282 - val_accuracy: 0.8090\n",
      "Epoch 153/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3817 - accuracy: 0.8284 - val_loss: 0.4307 - val_accuracy: 0.8034\n",
      "Epoch 154/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3808 - accuracy: 0.8312 - val_loss: 0.4284 - val_accuracy: 0.8090\n",
      "Epoch 155/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3818 - accuracy: 0.8340 - val_loss: 0.4270 - val_accuracy: 0.8034\n",
      "Epoch 156/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3825 - accuracy: 0.8326 - val_loss: 0.4326 - val_accuracy: 0.8034\n",
      "Epoch 157/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3810 - accuracy: 0.8340 - val_loss: 0.4308 - val_accuracy: 0.8146\n",
      "Epoch 158/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3802 - accuracy: 0.8312 - val_loss: 0.4266 - val_accuracy: 0.8034\n",
      "Epoch 159/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3820 - accuracy: 0.8354 - val_loss: 0.4247 - val_accuracy: 0.8090\n",
      "Epoch 160/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3813 - accuracy: 0.8326 - val_loss: 0.4336 - val_accuracy: 0.8090\n",
      "Epoch 161/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3845 - accuracy: 0.8298 - val_loss: 0.4290 - val_accuracy: 0.8146\n",
      "Epoch 162/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3812 - accuracy: 0.8368 - val_loss: 0.4231 - val_accuracy: 0.8090\n",
      "Epoch 163/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3804 - accuracy: 0.8340 - val_loss: 0.4276 - val_accuracy: 0.8034\n",
      "Epoch 164/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3810 - accuracy: 0.8354 - val_loss: 0.4390 - val_accuracy: 0.7978\n",
      "Epoch 165/300\n",
      "711/711 [==============================] - 0s 18us/sample - loss: 0.3828 - accuracy: 0.8298 - val_loss: 0.4307 - val_accuracy: 0.8090\n",
      "Epoch 166/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3829 - accuracy: 0.8312 - val_loss: 0.4307 - val_accuracy: 0.8034\n",
      "Epoch 167/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3812 - accuracy: 0.8284 - val_loss: 0.4321 - val_accuracy: 0.8034\n",
      "Epoch 168/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3836 - accuracy: 0.8383 - val_loss: 0.4396 - val_accuracy: 0.7978\n",
      "Epoch 169/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3827 - accuracy: 0.8312 - val_loss: 0.4210 - val_accuracy: 0.8090\n",
      "Epoch 170/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3796 - accuracy: 0.8312 - val_loss: 0.4385 - val_accuracy: 0.8034\n",
      "Epoch 171/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3815 - accuracy: 0.8340 - val_loss: 0.4298 - val_accuracy: 0.8090\n",
      "Epoch 172/300\n",
      "711/711 [==============================] - 0s 23us/sample - loss: 0.3811 - accuracy: 0.8340 - val_loss: 0.4331 - val_accuracy: 0.8034\n",
      "Epoch 173/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3823 - accuracy: 0.8340 - val_loss: 0.4286 - val_accuracy: 0.8090\n",
      "Epoch 174/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3798 - accuracy: 0.8340 - val_loss: 0.4276 - val_accuracy: 0.8090\n",
      "Epoch 175/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3833 - accuracy: 0.8298 - val_loss: 0.4378 - val_accuracy: 0.7978\n",
      "Epoch 176/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3812 - accuracy: 0.8326 - val_loss: 0.4281 - val_accuracy: 0.8090\n",
      "Epoch 177/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3824 - accuracy: 0.8298 - val_loss: 0.4315 - val_accuracy: 0.8090\n",
      "Epoch 178/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3786 - accuracy: 0.8298 - val_loss: 0.4337 - val_accuracy: 0.8034\n",
      "Epoch 179/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3789 - accuracy: 0.8383 - val_loss: 0.4286 - val_accuracy: 0.8034\n",
      "Epoch 180/300\n",
      "711/711 [==============================] - 0s 18us/sample - loss: 0.3771 - accuracy: 0.8354 - val_loss: 0.4311 - val_accuracy: 0.8034\n",
      "Epoch 181/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3784 - accuracy: 0.8354 - val_loss: 0.4370 - val_accuracy: 0.8090\n",
      "Epoch 182/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3776 - accuracy: 0.8368 - val_loss: 0.4296 - val_accuracy: 0.8034\n",
      "Epoch 183/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3788 - accuracy: 0.8354 - val_loss: 0.4317 - val_accuracy: 0.8034\n",
      "Epoch 184/300\n",
      "711/711 [==============================] - 0s 18us/sample - loss: 0.3767 - accuracy: 0.8383 - val_loss: 0.4359 - val_accuracy: 0.8034\n",
      "Epoch 185/300\n",
      "711/711 [==============================] - 0s 19us/sample - loss: 0.3777 - accuracy: 0.8340 - val_loss: 0.4303 - val_accuracy: 0.8090\n",
      "Epoch 186/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3771 - accuracy: 0.8368 - val_loss: 0.4340 - val_accuracy: 0.8090\n",
      "Epoch 187/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3784 - accuracy: 0.8368 - val_loss: 0.4341 - val_accuracy: 0.7978\n",
      "Epoch 188/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3805 - accuracy: 0.8256 - val_loss: 0.4346 - val_accuracy: 0.7978\n",
      "Epoch 189/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3817 - accuracy: 0.8326 - val_loss: 0.4343 - val_accuracy: 0.8034\n",
      "Epoch 190/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3819 - accuracy: 0.8354 - val_loss: 0.4355 - val_accuracy: 0.8034\n",
      "Epoch 191/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3771 - accuracy: 0.8368 - val_loss: 0.4346 - val_accuracy: 0.8090\n",
      "Epoch 192/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3774 - accuracy: 0.8354 - val_loss: 0.4330 - val_accuracy: 0.7978\n",
      "Epoch 193/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3789 - accuracy: 0.8368 - val_loss: 0.4311 - val_accuracy: 0.8090\n",
      "Epoch 194/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3767 - accuracy: 0.8354 - val_loss: 0.4379 - val_accuracy: 0.8034\n",
      "Epoch 195/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3783 - accuracy: 0.8340 - val_loss: 0.4376 - val_accuracy: 0.8034\n",
      "Epoch 196/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3770 - accuracy: 0.8368 - val_loss: 0.4330 - val_accuracy: 0.8034\n",
      "Epoch 197/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3821 - accuracy: 0.8298 - val_loss: 0.4358 - val_accuracy: 0.7865\n",
      "Epoch 198/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3770 - accuracy: 0.8354 - val_loss: 0.4361 - val_accuracy: 0.8034\n",
      "Epoch 199/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3784 - accuracy: 0.8326 - val_loss: 0.4359 - val_accuracy: 0.8090\n",
      "Epoch 200/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3769 - accuracy: 0.8340 - val_loss: 0.4386 - val_accuracy: 0.7978\n",
      "Epoch 201/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3759 - accuracy: 0.8368 - val_loss: 0.4310 - val_accuracy: 0.8090\n",
      "Epoch 202/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3750 - accuracy: 0.8354 - val_loss: 0.4396 - val_accuracy: 0.8034\n",
      "Epoch 203/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3748 - accuracy: 0.8340 - val_loss: 0.4382 - val_accuracy: 0.8090\n",
      "Epoch 204/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3764 - accuracy: 0.8354 - val_loss: 0.4329 - val_accuracy: 0.8034\n",
      "Epoch 205/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3761 - accuracy: 0.8312 - val_loss: 0.4403 - val_accuracy: 0.8034\n",
      "Epoch 206/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3749 - accuracy: 0.8368 - val_loss: 0.4370 - val_accuracy: 0.8034\n",
      "Epoch 207/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3750 - accuracy: 0.8354 - val_loss: 0.4330 - val_accuracy: 0.7978\n",
      "Epoch 208/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3741 - accuracy: 0.8368 - val_loss: 0.4373 - val_accuracy: 0.7978\n",
      "Epoch 209/300\n",
      "711/711 [==============================] - 0s 18us/sample - loss: 0.3743 - accuracy: 0.8354 - val_loss: 0.4390 - val_accuracy: 0.7978\n",
      "Epoch 210/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3750 - accuracy: 0.8368 - val_loss: 0.4337 - val_accuracy: 0.8034\n",
      "Epoch 211/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3762 - accuracy: 0.8340 - val_loss: 0.4468 - val_accuracy: 0.7978\n",
      "Epoch 212/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3739 - accuracy: 0.8354 - val_loss: 0.4350 - val_accuracy: 0.8034\n",
      "Epoch 213/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3754 - accuracy: 0.8312 - val_loss: 0.4324 - val_accuracy: 0.8090\n",
      "Epoch 214/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3793 - accuracy: 0.8368 - val_loss: 0.4361 - val_accuracy: 0.8034\n",
      "Epoch 215/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3754 - accuracy: 0.8340 - val_loss: 0.4475 - val_accuracy: 0.8034\n",
      "Epoch 216/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3754 - accuracy: 0.8354 - val_loss: 0.4315 - val_accuracy: 0.8034\n",
      "Epoch 217/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3787 - accuracy: 0.8284 - val_loss: 0.4331 - val_accuracy: 0.7978\n",
      "Epoch 218/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3748 - accuracy: 0.8340 - val_loss: 0.4296 - val_accuracy: 0.7978\n",
      "Epoch 219/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3726 - accuracy: 0.8368 - val_loss: 0.4411 - val_accuracy: 0.7978\n",
      "Epoch 220/300\n",
      "711/711 [==============================] - 0s 30us/sample - loss: 0.3783 - accuracy: 0.8298 - val_loss: 0.4478 - val_accuracy: 0.7978\n",
      "Epoch 221/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3771 - accuracy: 0.8354 - val_loss: 0.4297 - val_accuracy: 0.8034\n",
      "Epoch 222/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3732 - accuracy: 0.8368 - val_loss: 0.4406 - val_accuracy: 0.7978\n",
      "Epoch 223/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3756 - accuracy: 0.8368 - val_loss: 0.4409 - val_accuracy: 0.7978\n",
      "Epoch 224/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3748 - accuracy: 0.8368 - val_loss: 0.4285 - val_accuracy: 0.7978\n",
      "Epoch 225/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3745 - accuracy: 0.8354 - val_loss: 0.4464 - val_accuracy: 0.7978\n",
      "Epoch 226/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3747 - accuracy: 0.8340 - val_loss: 0.4353 - val_accuracy: 0.7978\n",
      "Epoch 227/300\n",
      "711/711 [==============================] - 0s 23us/sample - loss: 0.3736 - accuracy: 0.8340 - val_loss: 0.4383 - val_accuracy: 0.8090\n",
      "Epoch 228/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3742 - accuracy: 0.8354 - val_loss: 0.4420 - val_accuracy: 0.7978\n",
      "Epoch 229/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3747 - accuracy: 0.8354 - val_loss: 0.4378 - val_accuracy: 0.8090\n",
      "Epoch 230/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3720 - accuracy: 0.8397 - val_loss: 0.4391 - val_accuracy: 0.8034\n",
      "Epoch 231/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3738 - accuracy: 0.8368 - val_loss: 0.4486 - val_accuracy: 0.7865\n",
      "Epoch 232/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3782 - accuracy: 0.8312 - val_loss: 0.4295 - val_accuracy: 0.7921\n",
      "Epoch 233/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3815 - accuracy: 0.8340 - val_loss: 0.4356 - val_accuracy: 0.8034\n",
      "Epoch 234/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3776 - accuracy: 0.8326 - val_loss: 0.4389 - val_accuracy: 0.8146\n",
      "Epoch 235/300\n",
      "711/711 [==============================] - 0s 23us/sample - loss: 0.3723 - accuracy: 0.8368 - val_loss: 0.4515 - val_accuracy: 0.8034\n",
      "Epoch 236/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3744 - accuracy: 0.8340 - val_loss: 0.4344 - val_accuracy: 0.8034\n",
      "Epoch 237/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3747 - accuracy: 0.8397 - val_loss: 0.4320 - val_accuracy: 0.8034\n",
      "Epoch 238/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3741 - accuracy: 0.8383 - val_loss: 0.4461 - val_accuracy: 0.7921\n",
      "Epoch 239/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3718 - accuracy: 0.8326 - val_loss: 0.4356 - val_accuracy: 0.8090\n",
      "Epoch 240/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3717 - accuracy: 0.8383 - val_loss: 0.4370 - val_accuracy: 0.8034\n",
      "Epoch 241/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3720 - accuracy: 0.8368 - val_loss: 0.4361 - val_accuracy: 0.8034\n",
      "Epoch 242/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3729 - accuracy: 0.8383 - val_loss: 0.4425 - val_accuracy: 0.8034\n",
      "Epoch 243/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3724 - accuracy: 0.8340 - val_loss: 0.4389 - val_accuracy: 0.7978\n",
      "Epoch 244/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3713 - accuracy: 0.8397 - val_loss: 0.4398 - val_accuracy: 0.7978\n",
      "Epoch 245/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3724 - accuracy: 0.8368 - val_loss: 0.4366 - val_accuracy: 0.8090\n",
      "Epoch 246/300\n",
      "711/711 [==============================] - 0s 28us/sample - loss: 0.3731 - accuracy: 0.8368 - val_loss: 0.4413 - val_accuracy: 0.8034\n",
      "Epoch 247/300\n",
      "711/711 [==============================] - 0s 27us/sample - loss: 0.3732 - accuracy: 0.8397 - val_loss: 0.4440 - val_accuracy: 0.8034\n",
      "Epoch 248/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3738 - accuracy: 0.8368 - val_loss: 0.4356 - val_accuracy: 0.7978\n",
      "Epoch 249/300\n",
      "711/711 [==============================] - 0s 23us/sample - loss: 0.3723 - accuracy: 0.8383 - val_loss: 0.4487 - val_accuracy: 0.7921\n",
      "Epoch 250/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3738 - accuracy: 0.8354 - val_loss: 0.4345 - val_accuracy: 0.7978\n",
      "Epoch 251/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3712 - accuracy: 0.8383 - val_loss: 0.4450 - val_accuracy: 0.7978\n",
      "Epoch 252/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3716 - accuracy: 0.8397 - val_loss: 0.4429 - val_accuracy: 0.8034\n",
      "Epoch 253/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3723 - accuracy: 0.8368 - val_loss: 0.4419 - val_accuracy: 0.8034\n",
      "Epoch 254/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3747 - accuracy: 0.8397 - val_loss: 0.4410 - val_accuracy: 0.8034\n",
      "Epoch 255/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3790 - accuracy: 0.8354 - val_loss: 0.4482 - val_accuracy: 0.7921\n",
      "Epoch 256/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3719 - accuracy: 0.8383 - val_loss: 0.4376 - val_accuracy: 0.7978\n",
      "Epoch 257/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3739 - accuracy: 0.8326 - val_loss: 0.4439 - val_accuracy: 0.7978\n",
      "Epoch 258/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3738 - accuracy: 0.8340 - val_loss: 0.4511 - val_accuracy: 0.7865\n",
      "Epoch 259/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3720 - accuracy: 0.8383 - val_loss: 0.4377 - val_accuracy: 0.8034\n",
      "Epoch 260/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3717 - accuracy: 0.8340 - val_loss: 0.4418 - val_accuracy: 0.8090\n",
      "Epoch 261/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3695 - accuracy: 0.8383 - val_loss: 0.4430 - val_accuracy: 0.8034\n",
      "Epoch 262/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3735 - accuracy: 0.8368 - val_loss: 0.4478 - val_accuracy: 0.8034\n",
      "Epoch 263/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3717 - accuracy: 0.8340 - val_loss: 0.4413 - val_accuracy: 0.8034\n",
      "Epoch 264/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3702 - accuracy: 0.8368 - val_loss: 0.4409 - val_accuracy: 0.7978\n",
      "Epoch 265/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3702 - accuracy: 0.8298 - val_loss: 0.4476 - val_accuracy: 0.8034\n",
      "Epoch 266/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3740 - accuracy: 0.8312 - val_loss: 0.4358 - val_accuracy: 0.8090\n",
      "Epoch 267/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3706 - accuracy: 0.8397 - val_loss: 0.4470 - val_accuracy: 0.8034\n",
      "Epoch 268/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3705 - accuracy: 0.8397 - val_loss: 0.4425 - val_accuracy: 0.7978\n",
      "Epoch 269/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3703 - accuracy: 0.8397 - val_loss: 0.4402 - val_accuracy: 0.8034\n",
      "Epoch 270/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3700 - accuracy: 0.8354 - val_loss: 0.4494 - val_accuracy: 0.8034\n",
      "Epoch 271/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3721 - accuracy: 0.8354 - val_loss: 0.4359 - val_accuracy: 0.8090\n",
      "Epoch 272/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3705 - accuracy: 0.8383 - val_loss: 0.4510 - val_accuracy: 0.7978\n",
      "Epoch 273/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3696 - accuracy: 0.8354 - val_loss: 0.4420 - val_accuracy: 0.8034\n",
      "Epoch 274/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3700 - accuracy: 0.8425 - val_loss: 0.4419 - val_accuracy: 0.8090\n",
      "Epoch 275/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3706 - accuracy: 0.8411 - val_loss: 0.4477 - val_accuracy: 0.8034\n",
      "Epoch 276/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3747 - accuracy: 0.8312 - val_loss: 0.4506 - val_accuracy: 0.7978\n",
      "Epoch 277/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3705 - accuracy: 0.8354 - val_loss: 0.4445 - val_accuracy: 0.8034\n",
      "Epoch 278/300\n",
      "711/711 [==============================] - 0s 23us/sample - loss: 0.3698 - accuracy: 0.8368 - val_loss: 0.4375 - val_accuracy: 0.8090\n",
      "Epoch 279/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3698 - accuracy: 0.8397 - val_loss: 0.4474 - val_accuracy: 0.8034\n",
      "Epoch 280/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3703 - accuracy: 0.8368 - val_loss: 0.4452 - val_accuracy: 0.8034\n",
      "Epoch 281/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3698 - accuracy: 0.8397 - val_loss: 0.4397 - val_accuracy: 0.8034\n",
      "Epoch 282/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3704 - accuracy: 0.8368 - val_loss: 0.4478 - val_accuracy: 0.8034\n",
      "Epoch 283/300\n",
      "711/711 [==============================] - 0s 20us/sample - loss: 0.3694 - accuracy: 0.8383 - val_loss: 0.4465 - val_accuracy: 0.8034\n",
      "Epoch 284/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3688 - accuracy: 0.8354 - val_loss: 0.4453 - val_accuracy: 0.7978\n",
      "Epoch 285/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3721 - accuracy: 0.8368 - val_loss: 0.4430 - val_accuracy: 0.8034\n",
      "Epoch 286/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3699 - accuracy: 0.8411 - val_loss: 0.4510 - val_accuracy: 0.8034\n",
      "Epoch 287/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3695 - accuracy: 0.8298 - val_loss: 0.4426 - val_accuracy: 0.8090\n",
      "Epoch 288/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3701 - accuracy: 0.8368 - val_loss: 0.4452 - val_accuracy: 0.8090\n",
      "Epoch 289/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3686 - accuracy: 0.8411 - val_loss: 0.4435 - val_accuracy: 0.7921\n",
      "Epoch 290/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3686 - accuracy: 0.8397 - val_loss: 0.4436 - val_accuracy: 0.8034\n",
      "Epoch 291/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3681 - accuracy: 0.8397 - val_loss: 0.4448 - val_accuracy: 0.7978\n",
      "Epoch 292/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3682 - accuracy: 0.8425 - val_loss: 0.4530 - val_accuracy: 0.8034\n",
      "Epoch 293/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3700 - accuracy: 0.8383 - val_loss: 0.4498 - val_accuracy: 0.7978\n",
      "Epoch 294/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3713 - accuracy: 0.8368 - val_loss: 0.4382 - val_accuracy: 0.8034\n",
      "Epoch 295/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3684 - accuracy: 0.8411 - val_loss: 0.4530 - val_accuracy: 0.8034\n",
      "Epoch 296/300\n",
      "711/711 [==============================] - 0s 21us/sample - loss: 0.3706 - accuracy: 0.8354 - val_loss: 0.4527 - val_accuracy: 0.7921\n",
      "Epoch 297/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3692 - accuracy: 0.8397 - val_loss: 0.4397 - val_accuracy: 0.7978\n",
      "Epoch 298/300\n",
      "711/711 [==============================] - 0s 22us/sample - loss: 0.3702 - accuracy: 0.8383 - val_loss: 0.4469 - val_accuracy: 0.8090\n",
      "Epoch 299/300\n",
      "711/711 [==============================] - 0s 25us/sample - loss: 0.3722 - accuracy: 0.8298 - val_loss: 0.4561 - val_accuracy: 0.8034\n",
      "Epoch 300/300\n",
      "711/711 [==============================] - 0s 24us/sample - loss: 0.3690 - accuracy: 0.8383 - val_loss: 0.4495 - val_accuracy: 0.7978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21fe180af48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"model2.fit(\\n    X_train,\\n    y_train,\\n    validation_data=(X_test, y_test),\\n    batch_size=80,\\n    epochs=300,\\n    verbose=1,\\n)\";\n",
       "                var nbb_formatted_code = \"model2.fit(\\n    X_train,\\n    y_train,\\n    validation_data=(X_test, y_test),\\n    batch_size=80,\\n    epochs=300,\\n    verbose=1,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=80,\n",
    "    epochs=300,\n",
    "    verbose=1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Day 82 Lecture 1 Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
